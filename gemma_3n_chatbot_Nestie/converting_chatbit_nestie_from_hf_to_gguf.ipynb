{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaXdi3s4Bs0r"
      },
      "source": [
        "## Setting up and Dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "C0_sLL4A__t3",
        "outputId": "7b95a3c8-5afa-4399-a795-84be4e62b9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "14fafe875e16435bb3813511b4ec5e99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "Successfully installed transformers-4.55.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "bfdd027188ae4da39cce443c1f63f7f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes transformers accelerate\n",
        "!pip install --upgrade transformers\n",
        "!pip install huggingface_hub torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cvB51mZJhoJ"
      },
      "source": [
        "## Downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RclZK4G6BV4g",
        "outputId": "32bd2658-2650-4cb7-b532-39e08cdd9149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model_comversion\n"
          ]
        }
      ],
      "source": [
        "#loading into the model directory\n",
        "%cd /content/model_comversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tLbW1B9N-G19"
      },
      "outputs": [],
      "source": [
        "#importing necessary libaraies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tMuyezojBPqX",
        "outputId": "59f03673-da6f-47f2-aa20-03f061a53008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n",
            "README.md: 100% 608/608 [00:00<00:00, 4.44MB/s]\n",
            "\n",
            "chat_template.jinja: 1.53kB [00:00, 2.96MB/s]\n",
            "\n",
            "config.json: 5.35kB [00:00, 15.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/3.08G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "generation_config.json: 100% 215/215 [00:00<00:00, 1.58MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/2.82G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 1.57kB [00:00, 2.11MB/s]\n",
            "Fetching 15 files:   7% 1/15 [00:00<00:03,  4.02it/s]\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors.index.json: 159kB [00:00, 120MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "processor_config.json: 100% 98.0/98.0 [00:00<00:00, 597kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "preprocessor_config.json: 1.09kB [00:00, 1.25MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 777/777 [00:00<00:00, 5.38MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/33.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 1.20MB [00:00, 48.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model:   4% 170k/4.70M [00:00<00:11, 408kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 4.70M/4.70M [00:00<00:00, 9.00MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 33.4M/33.4M [00:00<00:00, 41.9MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 878k/3.08G [00:01<1:21:18, 631kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 858k/4.98G [00:01<2:57:21, 468kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 2.57M/4.98G [00:02<1:15:30, 1.10MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 4.87M/4.98G [00:07<2:12:56, 624kB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 7.09M/2.82G [00:09<1:00:55, 770kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 2.21M/3.08G [00:19<1:21:16, 631kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 4.87M/4.98G [00:19<2:12:56, 624kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 14.9M/2.82G [00:19<1:00:45, 770kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 69.3M/3.08G [00:36<26:33, 1.89MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 25.4M/2.82G [00:36<1:08:32, 680kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 136M/3.08G [00:44<13:57, 3.51MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 72.0M/4.98G [00:44<49:07, 1.67MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 203M/3.08G [00:45<07:42, 6.22MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 139M/4.98G [00:46<21:21, 3.78MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 270M/3.08G [00:46<04:57, 9.44MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   4% 206M/4.98G [00:47<12:12, 6.52MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 338M/3.08G [00:47<03:23, 13.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 405M/3.08G [00:48<02:24, 18.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 25.4M/2.82G [00:49<1:08:32, 680kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 273M/4.98G [00:51<09:17, 8.45MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 339M/4.98G [00:54<06:53, 11.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 472M/3.08G [00:54<02:51, 15.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 539M/3.08G [01:00<03:00, 14.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 406M/4.98G [01:09<06:47, 11.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 472M/4.98G [01:11<08:09, 9.22MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 539M/3.08G [01:19<03:00, 14.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  11% 539M/4.98G [01:29<08:02, 9.22MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 606M/4.98G [01:30<09:00, 8.09MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 740M/4.98G [01:43<08:00, 8.83MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 606M/3.08G [01:59<13:22, 3.08MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 740M/4.98G [01:59<08:00, 8.83MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 606M/3.08G [02:09<13:22, 3.08MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 807M/4.98G [02:10<12:09, 5.72MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 664M/3.08G [02:23<14:01, 2.87MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 807M/4.98G [02:29<12:09, 5.72MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 664M/3.08G [02:39<14:01, 2.87MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 874M/4.98G [02:40<16:14, 4.22MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 941M/4.98G [02:49<14:15, 4.72MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 941M/4.98G [02:59<14:15, 4.72MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.98G [03:04<14:17, 4.64MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 731M/3.08G [03:10<17:58, 2.17MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.98G [03:19<14:17, 4.64MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 798M/3.08G [03:21<13:53, 2.73MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.08G/4.98G [03:33<17:53, 3.64MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.08G/4.98G [03:33<17:53, 3.64MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 865M/3.08G [03:39<12:30, 2.95MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 865M/3.08G [03:49<12:30, 2.95MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.08G/4.98G [03:49<17:53, 3.64MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 932M/3.08G [03:52<10:28, 3.41MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.00G/3.08G [04:00<08:19, 4.16MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 85.4M/2.82G [04:08<2:20:37, 324kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.07G/3.08G [04:17<08:09, 4.11MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 85.4M/2.82G [04:19<2:20:37, 324kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.13G/3.08G [04:23<06:25, 5.04MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.98G [04:28<29:59, 2.13MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.15G/4.98G [04:29<29:48, 2.15MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.13G/3.08G [04:39<06:25, 5.04MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.15G/4.98G [04:39<29:48, 2.15MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.20G/3.08G [04:45<07:23, 4.23MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.98G [04:49<24:52, 2.52MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.20G/3.08G [04:59<07:23, 4.23MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.98G [04:59<24:52, 2.52MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.98G [05:00<19:09, 3.22MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.35G/4.98G [05:10<15:19, 3.95MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.98G [05:13<11:08, 5.34MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 1.27G/3.08G [05:24<10:18, 2.92MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.98G [05:29<11:08, 5.34MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 1.34G/3.08G [05:34<08:11, 3.55MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.98G [05:40<14:56, 3.90MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.55G/4.98G [05:48<12:10, 4.70MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 1.34G/3.08G [05:49<08:11, 3.55MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.98G [05:53<09:29, 5.91MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 1.40G/3.08G [06:05<09:20, 2.99MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.98G [06:09<09:29, 5.91MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 1.47G/3.08G [06:10<06:56, 3.86MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 1.54G/3.08G [06:19<05:39, 4.53MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.98G [06:23<13:58, 3.94MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.98G [06:23<09:35, 5.62MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 1.54G/3.08G [06:29<05:39, 4.53MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.98G [06:39<09:35, 5.62MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 1.60G/3.08G [06:40<06:06, 4.02MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  36% 1.82G/4.98G [06:41<10:52, 4.85MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 146M/2.82G [06:51<2:07:51, 349kB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.98G [06:52<09:56, 5.19MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 1.60G/3.08G [06:59<06:06, 4.02MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.98G [07:04<09:30, 5.32MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 1.67G/3.08G [07:04<06:37, 3.53MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 146M/2.82G [07:09<2:07:51, 349kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 1.74G/3.08G [07:14<05:24, 4.13MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 1.81G/3.08G [07:16<03:48, 5.58MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.98G [07:19<09:30, 5.32MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 1.87G/3.08G [07:23<03:06, 6.45MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  40% 2.02G/4.98G [07:32<12:39, 3.90MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 1.87G/3.08G [07:39<03:06, 6.45MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.98G [07:40<10:18, 4.68MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 1.94G/3.08G [07:40<03:30, 5.40MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 2.01G/3.08G [07:49<03:02, 5.87MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/4.98G [07:51<09:30, 4.96MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 2.07G/3.08G [07:58<02:41, 6.20MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.22G/4.98G [08:03<08:58, 5.13MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 2.07G/3.08G [08:09<02:41, 6.20MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 2.14G/3.08G [08:10<02:33, 6.11MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.29G/4.98G [08:13<08:10, 5.50MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 2.21G/3.08G [08:20<02:18, 6.28MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.98G [08:21<07:06, 6.16MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.98G [08:28<06:10, 6.91MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 2.27G/3.08G [08:34<02:21, 5.68MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.98G [08:39<06:10, 6.91MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 2.34G/3.08G [08:40<01:48, 6.77MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 2.49G/4.98G [08:40<06:25, 6.47MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 2.41G/3.08G [08:49<01:36, 6.97MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.98G [08:53<06:37, 6.10MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.98G [08:53<04:37, 8.50MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/4.98G [08:54<03:13, 11.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 2.47G/3.08G [08:54<01:15, 7.96MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.98G [08:55<02:23, 15.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.98G [09:05<03:12, 11.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.98G [09:09<02:49, 12.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 2.47G/3.08G [09:09<01:15, 7.96MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 2.54G/3.08G [09:10<01:24, 6.31MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 213M/2.82G [09:15<1:50:24, 394kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 2.61G/3.08G [09:19<01:11, 6.56MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.98G [09:19<02:49, 12.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  59% 2.96G/4.98G [09:25<04:23, 7.69MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 2.67G/3.08G [09:25<00:54, 7.43MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 213M/2.82G [09:29<1:50:24, 394kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 2.74G/3.08G [09:34<00:43, 7.66MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 280M/2.82G [09:34<1:09:45, 607kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  59% 2.96G/4.98G [09:39<04:23, 7.69MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.98G [09:40<05:03, 6.45MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 347M/2.82G [09:40<44:52, 919kB/s]  \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.98G [09:43<03:57, 7.95MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  15% 414M/2.82G [09:44<29:35, 1.36MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 2.74G/3.08G [09:49<00:43, 7.66MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 2.81G/3.08G [09:51<00:45, 5.95MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  17% 481M/2.82G [09:51<20:40, 1.89MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.98G [09:59<03:57, 7.95MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 2.88G/3.08G [10:00<00:32, 6.25MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 549M/2.82G [10:03<15:54, 2.38MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 616M/2.82G [10:04<10:50, 3.39MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 683M/2.82G [10:11<08:16, 4.31MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 750M/2.82G [10:14<06:11, 5.58MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 817M/2.82G [10:15<04:13, 7.91MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  31% 884M/2.82G [10:19<03:27, 9.32MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 2.88G/3.08G [10:19<00:32, 6.25MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 951M/2.82G [10:19<02:22, 13.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  36% 1.02G/2.82G [10:25<02:17, 13.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 1.09G/2.82G [10:25<01:36, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  41% 1.15G/2.82G [10:31<01:48, 15.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 2.94G/3.08G [10:31<00:33, 3.99MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.22G/2.82G [10:36<01:52, 14.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  46% 1.29G/2.82G [10:41<01:47, 14.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 1.35G/2.82G [10:45<01:39, 14.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 2.94G/3.08G [10:49<00:33, 3.99MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 1.42G/2.82G [10:51<01:44, 13.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 1.49G/2.82G [10:55<01:34, 14.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 1.55G/2.82G [10:56<01:04, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.11G/4.98G [10:57<16:41, 1.87MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 3.01G/3.08G [11:00<00:20, 3.27MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  57% 1.62G/2.82G [11:01<01:08, 17.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.17G/4.98G [11:04<11:28, 2.62MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  60% 1.69G/2.82G [11:04<01:01, 18.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.24G/4.98G [11:05<07:31, 3.85MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  62% 1.76G/2.82G [11:05<00:47, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 3.08G/3.08G [11:10<00:00, 4.59MB/s]\n",
            "Fetching 15 files:  40% 6/15 [11:10<17:38, 117.62s/it]\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 1.82G/2.82G [11:10<00:52, 18.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 1.89G/2.82G [11:16<00:59, 15.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.24G/4.98G [11:19<07:31, 3.85MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  69% 1.96G/2.82G [11:20<00:54, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 2.02G/2.82G [11:21<00:37, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 2.09G/2.82G [11:26<00:41, 17.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 2.15G/2.82G [11:26<00:28, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 2.22G/2.82G [11:27<00:19, 31.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.26G/4.98G [11:28<10:56, 2.63MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  67% 3.32G/4.98G [11:36<07:52, 3.51MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 2.28G/2.82G [11:36<00:34, 15.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.38G/4.98G [11:37<05:16, 5.05MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.45G/4.98G [11:41<03:46, 6.77MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  71% 3.51G/4.98G [11:45<02:55, 8.38MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.58G/4.98G [11:45<01:55, 12.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.65G/4.98G [11:46<01:21, 16.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 2.28G/2.82G [11:49<00:34, 15.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  75% 3.72G/4.98G [11:52<01:28, 14.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.78G/4.98G [11:55<01:14, 16.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  83% 2.35G/2.82G [11:55<01:01, 7.65MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.85G/4.98G [11:56<00:54, 20.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 3.91G/4.98G [12:01<00:59, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.98G/4.98G [12:07<01:06, 15.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  86% 2.42G/2.82G [12:07<00:58, 6.83MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 2.49G/2.82G [12:12<00:41, 8.05MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.05G/4.98G [12:16<01:18, 11.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.12G/4.98G [12:16<00:54, 15.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/4.98G [12:17<00:36, 22.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 2.55G/2.82G [12:17<00:29, 9.20MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.25G/4.98G [12:18<00:26, 27.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.32G/4.98G [12:23<00:31, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.38G/4.98G [12:28<00:34, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 2.55G/2.82G [12:29<00:29, 9.20MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.45G/4.98G [12:32<00:30, 17.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.52G/4.98G [12:36<00:27, 16.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.58G/4.98G [12:36<00:16, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.65G/4.98G [12:38<00:12, 25.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 2.62G/2.82G [12:38<00:34, 5.85MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.72G/4.98G [12:40<00:09, 27.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.79G/4.98G [12:41<00:05, 35.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.85G/4.98G [12:41<00:02, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 2.69G/2.82G [12:42<00:18, 7.26MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 2.75G/2.82G [12:46<00:07, 8.92MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.91G/4.98G [12:46<00:02, 26.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 2.82G/2.82G [12:47<00:00, 3.67MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.98G/4.98G [12:52<00:00, 6.45MB/s]\n",
            "Fetching 15 files: 100% 15/15 [12:52<00:00, 51.52s/it]\n"
          ]
        }
      ],
      "source": [
        "# downloading the model\n",
        "!python download.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VHsh_lkN-7C5",
        "outputId": "ed351872-484c-47e2-b554-fb4414fe8aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 11G\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Aug  6 13:39 .\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Aug  6 13:39 ..\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Aug  6 13:26 .cache\n",
            "4.0K -rw-r--r-- 1 root root 1.5K Aug  6 13:26 chat_template.jinja\n",
            "8.0K -rw-r--r-- 1 root root 5.3K Aug  6 13:26 config.json\n",
            "4.0K -rw-r--r-- 1 root root  215 Aug  6 13:26 generation_config.json\n",
            "4.0K -rw-r--r-- 1 root root 1.6K Aug  6 13:26 .gitattributes\n",
            "2.9G -rw-r--r-- 1 root root 2.9G Aug  6 13:37 model-00001-of-00003.safetensors\n",
            "4.7G -rw-r--r-- 1 root root 4.7G Aug  6 13:39 model-00002-of-00003.safetensors\n",
            "2.7G -rw-r--r-- 1 root root 2.7G Aug  6 13:39 model-00003-of-00003.safetensors\n",
            "156K -rw-r--r-- 1 root root 156K Aug  6 13:26 model.safetensors.index.json\n",
            "4.0K -rw-r--r-- 1 root root 1.1K Aug  6 13:26 preprocessor_config.json\n",
            "4.0K -rw-r--r-- 1 root root   98 Aug  6 13:26 processor_config.json\n",
            "4.0K -rw-r--r-- 1 root root  608 Aug  6 13:26 README.md\n",
            "4.0K -rw-r--r-- 1 root root  777 Aug  6 13:26 special_tokens_map.json\n",
            "1.2M -rw-r--r-- 1 root root 1.2M Aug  6 13:26 tokenizer_config.json\n",
            " 32M -rw-r--r-- 1 root root  32M Aug  6 13:26 tokenizer.json\n",
            "4.5M -rw-r--r-- 1 root root 4.5M Aug  6 13:26 tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "# verifying the model is downloaded\n",
        "!ls -lash vicuna-hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRRtzSdqCoYr"
      },
      "source": [
        "## Converting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ExuxTNb2-7Fj",
        "outputId": "2c108c4f-f321-42fc-c4a5-f23580ff26b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 58457, done.\u001b[K\n",
            "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 58457 (delta 109), reused 27 (delta 27), pack-reused 58255 (from 4)\u001b[K\n",
            "Receiving objects: 100% (58457/58457), 142.06 MiB | 17.67 MiB/s, done.\n",
            "Resolving deltas: 100% (42259/42259), done.\n"
          ]
        }
      ],
      "source": [
        "#cloning the llama.cpp repo\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "O5MzmQjA-7I8",
        "outputId": "09e51942-2f35-41f5-e33a-f065457d3cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Collecting numpy~=1.26.4 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.54.0)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4))\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting mistral-common>=1.8.3 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1))\n",
            "  Downloading mistral_common-1.8.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting torch~=2.2.1 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp~=3.9.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 1))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest~=8.3.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 2))\n",
            "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting huggingface_hub~=0.23.2 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 3))\n",
            "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n",
            "Collecting openai~=1.55.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 6))\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pandas~=2.2.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 7))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 8))\n",
            "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n",
            "Collecting wget~=3.2 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 10))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typer~=0.15.1 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 11))\n",
            "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.18.0)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers<5.0.0,>=4.45.1 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (4.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (11.3.0)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1))\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (6.6.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.20.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2025.7.14)\n",
            "Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1))\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (0.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
            "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=56ade34cdab3992a89828f02fd612cc564bd17079820cd7f30218d4d4951466e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, pytest, pycountry, protobuf, prometheus-client, numpy, click, torch, pandas, huggingface_hub, gguf, aiohttp, typer, tokenizers, pydantic-extra-types, openai, transformers, mistral-common\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.4.1\n",
            "    Uninstalling pytest-8.4.1:\n",
            "      Successfully uninstalled pytest-8.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus_client 0.22.1\n",
            "    Uninstalling prometheus_client-0.22.1:\n",
            "      Successfully uninstalled prometheus_client-0.22.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.34.1\n",
            "    Uninstalling huggingface-hub-0.34.1:\n",
            "      Successfully uninstalled huggingface-hub-0.34.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.12.14\n",
            "    Uninstalling aiohttp-3.12.14:\n",
            "      Successfully uninstalled aiohttp-3.12.14\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.16.0\n",
            "    Uninstalling typer-0.16.0:\n",
            "      Successfully uninstalled typer-0.16.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.97.1\n",
            "    Uninstalling openai-1.97.1:\n",
            "      Successfully uninstalled openai-1.97.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "gradio 5.38.2 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "datasets 4.0.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "peft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.5 click-8.1.8 gguf-0.17.1 huggingface_hub-0.23.5 mistral-common-1.8.3 numpy-1.26.4 openai-1.55.3 pandas-2.2.3 prometheus-client-0.20.0 protobuf-4.25.8 pycountry-24.6.1 pydantic-extra-types-2.10.5 pytest-8.3.5 tokenizers-0.20.3 torch-2.2.2+cpu transformers-4.46.3 typer-0.15.4 wget-3.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "27dd104d535541169dfb575c321e643b",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pandas",
                  "tokenizers",
                  "torch",
                  "torchgen",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# installing the required python libraries:\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4wRZHqoPBZ1j",
        "outputId": "baa37328-44bf-4577-ed51-679dd69ff449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n",
            "                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}]\n",
            "                             [--bigendian] [--use-temp-file] [--no-lazy]\n",
            "                             [--model-name MODEL_NAME] [--verbose]\n",
            "                             [--split-max-tensors SPLIT_MAX_TENSORS]\n",
            "                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n",
            "                             [--no-tensor-first-split] [--metadata METADATA]\n",
            "                             [--print-supported-models] [--remote] [--mmproj]\n",
            "                             [model]\n",
            "\n",
            "Convert a huggingface model to a GGML compatible file\n",
            "\n",
            "positional arguments:\n",
            "  model                 directory containing model file or huggingface\n",
            "                        repository ID (if --remote)\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --vocab-only          extract only the vocab\n",
            "  --outfile OUTFILE     path to write to; default: based on input. {ftype}\n",
            "                        will be replaced by the outtype.\n",
            "  --outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}\n",
            "                        output format - use f32 for float32, f16 for float16,\n",
            "                        bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for\n",
            "                        ternary, and auto for the highest-fidelity 16-bit\n",
            "                        float type depending on the first loaded tensor type\n",
            "  --bigendian           model is executed on big endian machine\n",
            "  --use-temp-file       use the tempfile library while processing (helpful\n",
            "                        when running out of memory, process killed)\n",
            "  --no-lazy             use more RAM by computing all outputs before writing\n",
            "                        (use in case lazy evaluation is broken)\n",
            "  --model-name MODEL_NAME\n",
            "                        name of the model\n",
            "  --verbose             increase output verbosity\n",
            "  --split-max-tensors SPLIT_MAX_TENSORS\n",
            "                        max tensors in each split\n",
            "  --split-max-size SPLIT_MAX_SIZE\n",
            "                        max size per split N(M|G)\n",
            "  --dry-run             only print out a split plan and exit, without writing\n",
            "                        any new files\n",
            "  --no-tensor-first-split\n",
            "                        do not add tensors to the first split (disabled by\n",
            "                        default)\n",
            "  --metadata METADATA   Specify the path for an authorship metadata override\n",
            "                        file\n",
            "  --print-supported-models\n",
            "                        Print the supported models\n",
            "  --remote              (Experimental) Read safetensors file remotely without\n",
            "                        downloading to disk. Config and tokenizer files will\n",
            "                        still be downloaded. To use this feature, you need to\n",
            "                        specify Hugging Face model repo name instead of a\n",
            "                        local directory. For example:\n",
            "                        'HuggingFaceTB/SmolLM2-1.7B-Instruct'. Note: To access\n",
            "                        gated repo, set HF_TOKEN environment variable to your\n",
            "                        Hugging Face token.\n",
            "  --mmproj              (Experimental) Export multimodal projector (mmproj)\n",
            "                        for vision models. This will only work on some vision\n",
            "                        models. A prefix 'mmproj-' will be added to the output\n",
            "                        file name.\n"
          ]
        }
      ],
      "source": [
        "!python llama.cpp/convert_hf_to_gguf.py -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5JNje9bQBZ7F",
        "outputId": "7d503c1d-9564-44be-86b3-eb7789de81e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: vicuna-hf\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "WARNING:hf-to-gguf:ignore tokens from 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262145: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262146: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262147: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262148: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262149: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262150: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262151: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262152: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262153: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262154: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262155: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262156: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262157: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262158: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262159: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262160: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262161: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262162: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262163: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262164: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262165: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262166: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262167: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262168: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262169: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262170: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262171: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262172: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262173: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262174: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262175: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262176: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262177: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262178: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262179: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262180: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262181: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262182: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262183: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262184: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262185: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262186: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262187: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262188: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262189: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262190: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262191: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262192: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262193: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262194: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262195: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262196: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262197: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262198: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262199: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262200: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262201: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262202: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262203: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262204: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262205: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262206: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262207: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262208: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262209: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262210: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262211: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262212: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262213: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262214: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262215: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262216: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262217: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262218: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262219: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262220: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262221: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262222: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262223: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262224: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262225: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262226: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262227: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262228: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262229: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262230: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262231: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262232: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262233: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262234: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262235: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262236: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262237: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262238: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262239: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262240: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262241: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262242: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262243: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262244: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262245: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262246: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262247: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262248: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262249: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262250: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262251: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262252: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262253: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262254: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262255: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262256: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262257: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262258: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262259: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262260: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262261: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262262: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262263: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262264: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262265: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262266: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262267: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262268: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262269: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262270: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262271: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262272: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262273: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262274: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262275: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262276: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262277: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262278: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262279: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262280: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262281: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262282: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262283: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262284: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262285: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262286: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262287: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262288: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262289: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262290: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262291: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262292: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262293: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262294: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262295: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262296: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262297: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262298: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262299: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262300: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262301: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262302: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262303: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262304: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262305: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262306: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262307: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262308: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262309: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262310: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262311: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262312: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262313: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262314: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262315: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262316: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262317: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262318: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262319: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262320: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262321: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262322: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262323: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262324: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262325: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262326: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262327: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262328: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262329: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262330: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262331: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262332: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262333: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262334: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262335: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262336: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262337: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262338: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262339: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262340: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262341: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262342: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262343: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262344: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262345: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262346: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262347: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262348: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262349: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262350: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262351: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262352: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262353: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262354: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262355: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262356: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262357: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262358: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262359: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262360: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262361: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262362: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262363: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262364: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262365: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262366: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262367: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262368: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262369: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262370: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262371: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262372: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262373: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262374: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262375: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262376: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262377: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262378: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262379: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262380: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262381: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262382: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262383: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262384: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262385: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262386: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262387: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262388: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262389: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262390: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262391: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262392: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262393: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262394: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262395: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262396: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262397: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262398: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262399: id is out of range, max=262143\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> F16, shape = {2048, 262144}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> F16, shape = {7680, 262144}\n",
            "INFO:hf-to-gguf:blk.0.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.0.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.0.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.0.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.0.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.0.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.1.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.1.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.1.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.1.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.10.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.10.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.10.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.10.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.10.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.11.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.11.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.11.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.11.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.15.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.15.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.15.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.15.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.15.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.16.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.16.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.16.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.16.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.16.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.20.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.20.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.20.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.20.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.20.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.20.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.20.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.21.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.21.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.21.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.21.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.21.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.25.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.25.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.25.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.25.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.25.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.25.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.25.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.25.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.26.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.26.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.26.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.26.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.26.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.5.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.5.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.5.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.5.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.5.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.6.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.6.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.6.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.6.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> F16, shape = {2048, 7680}\n",
            "INFO:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.12.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.12.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.12.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.12.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.12.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.13.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.13.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.13.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.13.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.13.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.14.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.14.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.14.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.14.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.14.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.16.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.17.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.17.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.17.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.17.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.17.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.17.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.18.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.18.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.18.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.18.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.18.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.18.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.18.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.19.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.19.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.19.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.19.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.19.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.19.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.19.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.2.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.2.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.2.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.2.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.2.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.21.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.22.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.22.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.22.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.22.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.22.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.22.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.22.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.23.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.23.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.23.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.23.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.23.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.23.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.23.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.23.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.24.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.24.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.24.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.24.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.24.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.24.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.24.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.24.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.26.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.26.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.27.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.27.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.27.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.27.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.27.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.27.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.27.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.28.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.28.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.28.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.28.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.28.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.28.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.28.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.28.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.29.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.29.altup_router.weight,        torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.29.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.29.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.laurel_l.weight,            torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.29.laurel_r.weight,            torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.29.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.29.inp_gate.weight,            torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.29.proj.weight,                torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.3.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.3.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.3.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.3.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.3.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.4.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.4.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.4.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.4.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.4.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.7.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.7.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.7.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.7.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.7.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.8.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.8.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.8.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.8.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.8.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.9.altup_router.weight,         torch.bfloat16 --> F16, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.9.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.9.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.laurel_l.weight,             torch.bfloat16 --> F16, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.9.laurel_r.weight,             torch.bfloat16 --> F16, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.9.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.inp_gate.weight,             torch.bfloat16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.proj.weight,                 torch.bfloat16 --> F16, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:hf-to-gguf:ignore tokens from 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262145: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262146: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262147: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262148: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262149: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262150: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262151: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262152: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262153: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262154: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262155: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262156: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262157: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262158: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262159: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262160: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262161: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262162: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262163: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262164: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262165: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262166: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262167: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262168: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262169: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262170: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262171: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262172: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262173: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262174: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262175: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262176: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262177: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262178: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262179: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262180: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262181: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262182: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262183: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262184: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262185: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262186: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262187: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262188: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262189: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262190: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262191: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262192: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262193: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262194: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262195: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262196: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262197: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262198: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262199: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262200: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262201: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262202: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262203: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262204: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262205: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262206: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262207: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262208: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262209: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262210: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262211: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262212: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262213: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262214: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262215: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262216: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262217: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262218: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262219: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262220: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262221: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262222: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262223: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262224: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262225: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262226: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262227: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262228: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262229: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262230: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262231: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262232: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262233: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262234: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262235: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262236: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262237: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262238: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262239: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262240: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262241: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262242: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262243: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262244: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262245: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262246: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262247: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262248: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262249: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262250: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262251: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262252: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262253: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262254: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262255: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262256: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262257: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262258: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262259: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262260: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262261: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262262: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262263: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262264: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262265: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262266: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262267: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262268: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262269: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262270: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262271: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262272: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262273: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262274: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262275: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262276: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262277: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262278: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262279: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262280: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262281: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262282: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262283: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262284: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262285: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262286: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262287: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262288: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262289: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262290: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262291: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262292: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262293: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262294: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262295: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262296: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262297: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262298: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262299: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262300: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262301: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262302: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262303: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262304: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262305: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262306: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262307: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262308: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262309: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262310: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262311: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262312: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262313: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262314: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262315: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262316: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262317: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262318: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262319: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262320: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262321: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262322: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262323: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262324: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262325: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262326: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262327: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262328: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262329: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262330: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262331: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262332: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262333: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262334: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262335: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262336: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262337: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262338: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262339: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262340: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262341: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262342: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262343: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262344: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262345: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262346: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262347: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262348: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262349: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262350: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262351: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262352: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262353: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262354: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262355: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262356: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262357: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262358: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262359: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262360: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262361: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262362: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262363: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262364: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262365: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262366: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262367: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262368: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262369: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262370: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262371: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262372: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262373: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262374: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262375: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262376: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262377: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262378: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262379: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262380: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262381: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262382: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262383: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262384: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262385: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262386: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262387: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262388: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262389: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262390: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262391: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262392: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262393: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262394: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262395: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262396: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262397: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262398: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262399: id is out of range, max=262143\n",
            "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 106\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{ '<start_of_turn>model\n",
            "' }}\n",
            "{%- endif -%}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:vicuna-13b-v1.5.gguf: n_tensors = 727, total_size = 8.9G\n",
            "Writing:  13% 1.12G/8.91G [00:50<04:29, 28.9Mbyte/s]^C\n"
          ]
        }
      ],
      "source": [
        "#Converting the HF model to GGUF model\n",
        "!python llama.cpp/convert_hf_to_gguf.py vicuna-hf \\\n",
        "  --outfile vicuna-13b-v1.5.gguf\n",
        "#\\\n",
        "#  --outtype f16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KuMA0CkBZ-R",
        "outputId": "8222058e-70a6-4399-ebc2-ec2cea36af7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1G -rw-r--r-- 1 root root 1.1G Aug  6 13:47 vicuna-13b-v1.5.gguf\n"
          ]
        }
      ],
      "source": [
        "#Verifying the GGUF model was created\n",
        "!ls -lash vicuna-13b-v1.5.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/model_conversion/llama.cpp\n",
        "#!cmake /content/model_conversion/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WOBqv_rRUVeR",
        "outputId": "a3347de3-ce78-4255-9118-002cff104974"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model_conversion/llama.cpp\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6098\n",
            "-- ggml commit:  22414532\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.3s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/model_conversion/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRExQs8QqQat",
        "outputId": "9ac7c67b-96c2-4f0c-dee7-dcd33e05afed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model_comversion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp\n",
        "!mkdir build && cd build\n",
        "!cmake llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bA9KKtHmXqvA",
        "outputId": "08bc4912-8852-410c-b612-8372029a4277"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘build’: File exists\n",
            "\u001b[33mCMake Warning:\n",
            "  Ignoring extra path from command line:\n",
            "\n",
            "   \".\"\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Error: The source directory \"/content/model_comversion\" does not appear to contain CMakeLists.txt.\n",
            "Specify --help for usage, or press the help button on the CMake GUI.\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6101\n",
            "-- ggml commit:  0d883154\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (2.9s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/model_comversion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake .\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SgW-SVo3qiIZ",
        "outputId": "d4dbbde2-20e2-4400-ebbb-1f7d08d73428"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6101\n",
            "-- ggml commit:  0d883154\n",
            "-- Configuring done (0.4s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/model_comversion\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 11%] Built target ggml-cpu\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 11%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 22%] Built target llama\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 22%] Built target build_info\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 28%] Built target common\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 28%] Built target test-tokenizer-0\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 29%] Built target test-sampling\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 30%] Built target test-grammar-parser\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 31%] Built target test-grammar-integration\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 32%] Built target test-llama-grammar\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 33%] Built target test-chat\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 35%] Built target test-json-schema-to-grammar\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 36%] Built target test-quantize-stats\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 36%] Built target test-gbnf-validator\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 37%] Built target test-tokenizer-1-bpe\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 38%] Built target test-tokenizer-1-spm\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 39%] Built target test-chat-parser\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 41%] Built target test-chat-template\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 42%] Built target test-json-partial\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 43%] Built target test-log\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 44%] Built target test-regex-partial\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 46%] Built target test-thread-safety\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 47%] Built target test-arg-parser\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 49%] Built target test-gguf\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 50%] Built target test-backend-ops\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 51%] Built target test-model-load-cancel\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 53%] Built target test-autorelease\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 54%] Built target test-barrier\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 56%] Built target test-quantize-fns\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 57%] Built target test-quantize-perf\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 58%] Built target test-rope\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 60%] Built target mtmd\n",
            "[ 61%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 61%] Built target test-mtmd-c-api\n",
            "[ 61%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 62%] Built target test-c\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 63%] Built target llama-batched\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 63%] Built target llama-embedding\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 64%] Built target llama-eval-callback\n",
            "[ 64%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 64%] Built target sha256\n",
            "[ 65%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 65%] Built target xxhash\n",
            "[ 66%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 66%] Built target sha1\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 67%] Built target llama-gguf-hash\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 68%] Built target llama-gguf\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 69%] Built target llama-gritlm\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 70%] Built target llama-lookahead\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 71%] Built target llama-lookup\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 72%] Built target llama-lookup-create\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 73%] Built target llama-lookup-merge\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 73%] Built target llama-lookup-stats\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 74%] Built target llama-parallel\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 75%] Built target llama-passkey\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 76%] Built target llama-retrieval\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 77%] Built target llama-save-load-state\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 77%] Built target llama-simple\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 78%] Built target llama-simple-chat\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 79%] Built target llama-speculative\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 80%] Built target llama-speculative-simple\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 80%] Built target llama-gen-docs\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 81%] Built target llama-finetune\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 82%] Built target llama-diffusion-cli\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 83%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 84%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 84%] Built target llama-vdot\n",
            "[ 84%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 84%] Built target llama-q8dot\n",
            "[ 84%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 84%] Built target llama-batched-bench\n",
            "[ 84%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 85%] Built target llama-gguf-split\n",
            "[ 85%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 86%] Built target llama-imatrix\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 87%] Built target llama-bench\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 88%] Built target llama-cli\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 89%] Built target llama-perplexity\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 90%] Built target llama-quantize\n",
            "[ 91%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 91%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 92%] Built target llama-server\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 93%] Built target llama-run\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 94%] Built target llama-tokenize\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 94%] Built target llama-tts\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 95%] Built target llama-gemma3-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 96%] Built target llama-minicpmv-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] Built target llama-qwen2vl-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 99%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[100%] Built target llama-export-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls -lh /content/sample_data/risk_predictor_ollama_model_tq2_0/vicuna-13b-v1.5.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spiJ7YW4gN2x",
        "outputId": "2acbaf65-5ea9-4ff5-9dfc-206b6228b865"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.1G Aug  6 09:12 /content/sample_data/risk_predictor_ollama_model_tq2_0/vicuna-13b-v1.5.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"%cd /content/sample_data\n",
        "!git clone https://huggingface.co/TechBuz/risk_predictor_ollama_model_tq2_0\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zXf5HUItgXPJ",
        "outputId": "84123872-2826-4f37-cf37-5ae70a740039"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample_data\n",
            "Cloning into 'risk_predictor_ollama_model_tq2_0'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 2.16 KiB | 738.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./bin/llama-quantize \\content/model_comversion/vicuna-13b-v1.5.gguf\\content/model_conversion/output-model-q5_0.gguf Q4_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YWqMxF18bPx5",
        "outputId": "cf649f4e-559f-40c9-b45c-7f4428ce6bd6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 6101 (0d883154)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'content/model_comversion/vicuna-13b-v1.5.ggufcontent/model_conversion/output-model-q5_0.gguf' to 'content/model_comversion/vicuna-13b-v1.5.ggufcontent/model_conversion/ggml-model-Q4_0.gguf' as Q4_0\n",
            "gguf_init_from_file: failed to open GGUF file 'content/model_comversion/vicuna-13b-v1.5.ggufcontent/model_conversion/output-model-q5_0.gguf'\n",
            "llama_model_quantize: failed to quantize: llama_model_loader: failed to load model from content/model_comversion/vicuna-13b-v1.5.ggufcontent/model_conversion/output-model-q5_0.gguf\n",
            "main: failed to quantize model from 'content/model_comversion/vicuna-13b-v1.5.ggufcontent/model_conversion/output-model-q5_0.gguf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./bin/llama-quantize \\\n",
        "/content/model_comversion/vicuna-13b-v1.5.gguf \\\n",
        "/content/model_conversion/output-model-q5_0.gguf \\\n",
        "Q4_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R92nyoIN6FL",
        "outputId": "a5fbddac-49a1-4aa9-e8c8-8c024f78302b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 6101 (0d883154)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model_comversion/vicuna-13b-v1.5.gguf' to '/content/model_conversion/output-model-q5_0.gguf' as Q4_0\n",
            "llama_model_quantize: failed to quantize: tensor 'per_layer_token_embd.weight' data is not within the file bounds, model is corrupted or incomplete\n",
            "main: failed to quantize model from '/content/model_comversion/vicuna-13b-v1.5.gguf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLWhPUW9CsgU"
      },
      "source": [
        "## Pushing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "ef64386728ec4ce9b59633dad7511022",
            "54a7b743eab147349cf8be05d0c7df8c",
            "b1b811fc941d4225935294dc3394eeca",
            "4c89f8d904704ca2a0b764dc7f73cd41",
            "d9b44d695e194337878960f5f1ac755f",
            "9eb7e485753e46d0a60e9586bc0fbb96",
            "db7e0b1cad5340fcb421bc8fadfe43fd",
            "00ec7b8abc4e4a39b3f4038244535556",
            "d22b4aa795d349bcaf25800016638242",
            "36117670433440c188c529742bcef714",
            "f9ae4895401147698f2dcf001b9c96ed",
            "0e4e0c8c85af4ab9a813021428ad00e7",
            "5df16f46c7574872b6c867ccb02395c6",
            "47664911106b4d0db855c43bd9adfd8a",
            "2bc51d4f7b4243659c4730c4783c90d1",
            "ee5b15f6aa5f4b678905651e3414bb15",
            "aebe7ca1ac9f41c7898df9b7ae77c862",
            "8f6d2330d7e143a48412a352e69e23da",
            "59cc3841d9094d9fbfe7c7b4a397f0de",
            "3d7e5d9bee6f4faeb811d8363d958515",
            "65be69fa48334619a9c27ab6a1cdd91e",
            "35d2ddb435734a4aaddce37157ca60bd",
            "c283d9944089442a9ff1d7057fc746e6",
            "467209da66464f95bd19a985a3deeedb",
            "1c2d2fd8cf004da7867e5422a934ae0a",
            "89c1077ad1f54061b7b693345a70d877",
            "4f82cfa2c8a2435eb126acda7fdeb6c6",
            "9b7d818ddba143bda19dd7fc8303fced",
            "628e6143ab23449e982ffd35934f1cb1",
            "1e01f6f782624a29b5d33e4af21ced53",
            "43512a8f3cec4006af8d6888356202f1",
            "58515c17cb2b465099829186766aac28",
            "d6cfe866160545719757199b2bfc9bf7"
          ]
        },
        "id": "o-EI6t_fBjsa",
        "outputId": "788444e1-fd3f-425c-cb53-fd9497b5fb73",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef64386728ec4ce9b59633dad7511022"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e4e0c8c85af4ab9a813021428ad00e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  vicuna-13b-v1.5.gguf                  :   1%|          | 6.38MB / 1.09GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c283d9944089442a9ff1d7057fc746e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/TechBuz/risk_predictor_ollama_model_tq2_0/commit/b39420153fc8d8f046cea44799cd45bb7729edfc', commit_message='Upload vicuna-13b-v1.5.gguf with huggingface_hub', commit_description='', oid='b39420153fc8d8f046cea44799cd45bb7729edfc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/TechBuz/risk_predictor_ollama_model_tq2_0', endpoint='https://huggingface.co', repo_type='model', repo_id='TechBuz/risk_predictor_ollama_model_tq2_0'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Pushing the GGUF model to HuggingFace\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"TechBuz/risk_predictor_ollama_model_tq2_0\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"vicuna-13b-v1.5.gguf\",\n",
        "    path_in_repo=\"vicuna-13b-v1.5.gguf\",\n",
        "    repo_id=model_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2VgabVxwTwn",
        "outputId": "43d60d81-ff48-4052-cc50-c2e014d0d99a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/model_conversion/vicuna-hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OgC7MxPpQigu",
        "outputId": "3dc4b6a3-8675-4b20-cfbd-810f64e55535"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_template.jinja\t\t  preprocessor_config.json\n",
            "config.json\t\t\t  processor_config.json\n",
            "generation_config.json\t\t  README.md\n",
            "model-00001-of-00003.safetensors  special_tokens_map.json\n",
            "model-00002-of-00003.safetensors  tokenizer_config.json\n",
            "model-00003-of-00003.safetensors  tokenizer.json\n",
            "model.safetensors.index.json\t  tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # ⬅️ Enable offloading safely\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    \"TechBuz/gemma-3N-risk_predictor\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"  # ⬅️ Let Hugging Face place layers smartly (GPU/CPU)\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"TechBuz/gemma-3N-risk_predictor\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "5535f3e310e84c6991abdb62876952fa",
            "b87b43c805c2411cbdf357193913e211",
            "1832db395c0e4f37b508a51242dfdaf1",
            "e83565cee9b24837afbec3ce376e0287",
            "5f4b74203daa4bee8e3b4de6f573c043",
            "84b2cc77dedc4d1fb5361197e818c6a9",
            "2a4cdda2166b48888ade12a52c809b30",
            "02cda91de8d7481a951b852c8e96b22e",
            "bd27a8846a424d22beb88ffa2f88192d",
            "a11b0483ae054066aa30926ee7977cf6",
            "b59e0454a7454e42a58f8d6df6d72212",
            "1ab6860ed13f49a59b283aa9d660e1a2",
            "1bf5afded96f4785a1abe8ed57e36723",
            "83cecc9479fa430c9af1c476494e8355",
            "a4fe171026874472807a721bf83a1078",
            "bafd84d98e64446b9ee99480a9181f34",
            "a0669bff111c4b9286eac9e0ba8fc284",
            "6819ade9c2e34532994b866ebea185bc",
            "2616fab78bc74783bb7139640d625845",
            "52f1b3713c464aaaae8ebd4d0d43ec44",
            "97007f43a2824577b44d7482a7e4290c",
            "f35cbbe4ae95456cb4510776ea8c3be2"
          ]
        },
        "id": "3WRZsHBHwE0T",
        "outputId": "b2f87a49-db38-4b45-88c8-2972250e0e16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5535f3e310e84c6991abdb62876952fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ab6860ed13f49a59b283aa9d660e1a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPGll1xNxG84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMjH3g8rxGyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True: # Change to True to upload finetune\n",
        "    model.push_to_hub(\n",
        "        \"TechBuz/quant_gemma-3N-finetun_risk_pred\", processor,\n",
        "        token = \"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\"\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514,
          "referenced_widgets": [
            "e8c12a02391a4ca98b0cc23187f17b52",
            "6a300627db9a4a50913df74f0d641a1d",
            "7f6a63d2969d4bb68716ffe89b8a7a05",
            "3ce5bfad0650428f83cc81f3be063b09",
            "963398858e8f4ac1bfffcd1d39bffb2c",
            "cd4b880bc019409ea95baf79374f37e5",
            "c0f179f61de94ce6a51498a81cdf9548",
            "653c3ecd2d864fd7919cef52297458c3",
            "55f1915f76db429fa9274496ea4b4694",
            "b7a08324df7447ad87dfa724b59a9f54",
            "b74df2a377934a47bd1d164a2e182a0a"
          ]
        },
        "id": "6RVvFfYzw8MA",
        "outputId": "56372dfa-03a3-46ec-8e04-b2e5906afc1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3961: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8c12a02391a4ca98b0cc23187f17b52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot access accelerator device when none is available.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1265555324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Change to True to upload finetune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     model.push_to_hub(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"TechBuz/quant_gemma-3N-finetun_risk_pred\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"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\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4216\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_memory_footprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_buffers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 )\n\u001b[1;32m    979\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_shard_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_serialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_serialization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;31m# Update model card if needed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   4152\u001b[0m                         \u001b[0;31m# update state dict with onloaded parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m                         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m                         \u001b[0mshard_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state_dict_from_offload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m                 \u001b[0;31m# assign shard to be the completed state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mget_state_dict_from_offload\u001b[0;34m(module, module_name, state_dict, device_to_put_offload)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[0;31m# assign the device to which the offloaded parameters will be sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0malign_module_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_to_put_offload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\".{m_key}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36malign_module_device\u001b[0;34m(module, execution_device)\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2181\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2182\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;31m# quantize module that are going to stay on the cpu so that we offload quantized weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Int8Params\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                     \u001b[0mnew_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mnew_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"meta\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot access accelerator device when none is available."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef64386728ec4ce9b59633dad7511022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54a7b743eab147349cf8be05d0c7df8c",
              "IPY_MODEL_b1b811fc941d4225935294dc3394eeca",
              "IPY_MODEL_4c89f8d904704ca2a0b764dc7f73cd41"
            ],
            "layout": "IPY_MODEL_d9b44d695e194337878960f5f1ac755f"
          }
        },
        "54a7b743eab147349cf8be05d0c7df8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eb7e485753e46d0a60e9586bc0fbb96",
            "placeholder": "​",
            "style": "IPY_MODEL_db7e0b1cad5340fcb421bc8fadfe43fd",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "b1b811fc941d4225935294dc3394eeca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ec7b8abc4e4a39b3f4038244535556",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d22b4aa795d349bcaf25800016638242",
            "value": 1
          }
        },
        "4c89f8d904704ca2a0b764dc7f73cd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36117670433440c188c529742bcef714",
            "placeholder": "​",
            "style": "IPY_MODEL_f9ae4895401147698f2dcf001b9c96ed",
            "value": " 1.09GB / 1.09GB,  102MB/s  "
          }
        },
        "d9b44d695e194337878960f5f1ac755f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb7e485753e46d0a60e9586bc0fbb96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db7e0b1cad5340fcb421bc8fadfe43fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00ec7b8abc4e4a39b3f4038244535556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d22b4aa795d349bcaf25800016638242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36117670433440c188c529742bcef714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ae4895401147698f2dcf001b9c96ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e4e0c8c85af4ab9a813021428ad00e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5df16f46c7574872b6c867ccb02395c6",
              "IPY_MODEL_47664911106b4d0db855c43bd9adfd8a",
              "IPY_MODEL_2bc51d4f7b4243659c4730c4783c90d1"
            ],
            "layout": "IPY_MODEL_ee5b15f6aa5f4b678905651e3414bb15"
          }
        },
        "5df16f46c7574872b6c867ccb02395c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aebe7ca1ac9f41c7898df9b7ae77c862",
            "placeholder": "​",
            "style": "IPY_MODEL_8f6d2330d7e143a48412a352e69e23da",
            "value": "New Data Upload                         : 100%"
          }
        },
        "47664911106b4d0db855c43bd9adfd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59cc3841d9094d9fbfe7c7b4a397f0de",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d7e5d9bee6f4faeb811d8363d958515",
            "value": 1
          }
        },
        "2bc51d4f7b4243659c4730c4783c90d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65be69fa48334619a9c27ab6a1cdd91e",
            "placeholder": "​",
            "style": "IPY_MODEL_35d2ddb435734a4aaddce37157ca60bd",
            "value": " 77.5MB / 77.5MB, 1.72MB/s  "
          }
        },
        "ee5b15f6aa5f4b678905651e3414bb15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aebe7ca1ac9f41c7898df9b7ae77c862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6d2330d7e143a48412a352e69e23da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59cc3841d9094d9fbfe7c7b4a397f0de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3d7e5d9bee6f4faeb811d8363d958515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65be69fa48334619a9c27ab6a1cdd91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d2ddb435734a4aaddce37157ca60bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c283d9944089442a9ff1d7057fc746e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_467209da66464f95bd19a985a3deeedb",
              "IPY_MODEL_1c2d2fd8cf004da7867e5422a934ae0a",
              "IPY_MODEL_89c1077ad1f54061b7b693345a70d877"
            ],
            "layout": "IPY_MODEL_4f82cfa2c8a2435eb126acda7fdeb6c6"
          }
        },
        "467209da66464f95bd19a985a3deeedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b7d818ddba143bda19dd7fc8303fced",
            "placeholder": "​",
            "style": "IPY_MODEL_628e6143ab23449e982ffd35934f1cb1",
            "value": "  vicuna-13b-v1.5.gguf                  : 100%"
          }
        },
        "1c2d2fd8cf004da7867e5422a934ae0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e01f6f782624a29b5d33e4af21ced53",
            "max": 1086788000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43512a8f3cec4006af8d6888356202f1",
            "value": 1086788000
          }
        },
        "89c1077ad1f54061b7b693345a70d877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58515c17cb2b465099829186766aac28",
            "placeholder": "​",
            "style": "IPY_MODEL_d6cfe866160545719757199b2bfc9bf7",
            "value": " 1.09GB / 1.09GB            "
          }
        },
        "4f82cfa2c8a2435eb126acda7fdeb6c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b7d818ddba143bda19dd7fc8303fced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "628e6143ab23449e982ffd35934f1cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e01f6f782624a29b5d33e4af21ced53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43512a8f3cec4006af8d6888356202f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58515c17cb2b465099829186766aac28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6cfe866160545719757199b2bfc9bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5535f3e310e84c6991abdb62876952fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b87b43c805c2411cbdf357193913e211",
              "IPY_MODEL_1832db395c0e4f37b508a51242dfdaf1",
              "IPY_MODEL_e83565cee9b24837afbec3ce376e0287"
            ],
            "layout": "IPY_MODEL_5f4b74203daa4bee8e3b4de6f573c043"
          }
        },
        "b87b43c805c2411cbdf357193913e211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b2cc77dedc4d1fb5361197e818c6a9",
            "placeholder": "​",
            "style": "IPY_MODEL_2a4cdda2166b48888ade12a52c809b30",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1832db395c0e4f37b508a51242dfdaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02cda91de8d7481a951b852c8e96b22e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd27a8846a424d22beb88ffa2f88192d",
            "value": 3
          }
        },
        "e83565cee9b24837afbec3ce376e0287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a11b0483ae054066aa30926ee7977cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_b59e0454a7454e42a58f8d6df6d72212",
            "value": " 3/3 [00:52&lt;00:00, 17.37s/it]"
          }
        },
        "5f4b74203daa4bee8e3b4de6f573c043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b2cc77dedc4d1fb5361197e818c6a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4cdda2166b48888ade12a52c809b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02cda91de8d7481a951b852c8e96b22e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd27a8846a424d22beb88ffa2f88192d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a11b0483ae054066aa30926ee7977cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b59e0454a7454e42a58f8d6df6d72212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ab6860ed13f49a59b283aa9d660e1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bf5afded96f4785a1abe8ed57e36723",
              "IPY_MODEL_83cecc9479fa430c9af1c476494e8355",
              "IPY_MODEL_a4fe171026874472807a721bf83a1078"
            ],
            "layout": "IPY_MODEL_bafd84d98e64446b9ee99480a9181f34"
          }
        },
        "1bf5afded96f4785a1abe8ed57e36723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0669bff111c4b9286eac9e0ba8fc284",
            "placeholder": "​",
            "style": "IPY_MODEL_6819ade9c2e34532994b866ebea185bc",
            "value": "generation_config.json: 100%"
          }
        },
        "83cecc9479fa430c9af1c476494e8355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2616fab78bc74783bb7139640d625845",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52f1b3713c464aaaae8ebd4d0d43ec44",
            "value": 215
          }
        },
        "a4fe171026874472807a721bf83a1078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97007f43a2824577b44d7482a7e4290c",
            "placeholder": "​",
            "style": "IPY_MODEL_f35cbbe4ae95456cb4510776ea8c3be2",
            "value": " 215/215 [00:00&lt;00:00, 9.14kB/s]"
          }
        },
        "bafd84d98e64446b9ee99480a9181f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0669bff111c4b9286eac9e0ba8fc284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6819ade9c2e34532994b866ebea185bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2616fab78bc74783bb7139640d625845": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f1b3713c464aaaae8ebd4d0d43ec44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97007f43a2824577b44d7482a7e4290c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35cbbe4ae95456cb4510776ea8c3be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8c12a02391a4ca98b0cc23187f17b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a300627db9a4a50913df74f0d641a1d",
              "IPY_MODEL_7f6a63d2969d4bb68716ffe89b8a7a05",
              "IPY_MODEL_3ce5bfad0650428f83cc81f3be063b09"
            ],
            "layout": "IPY_MODEL_963398858e8f4ac1bfffcd1d39bffb2c"
          }
        },
        "6a300627db9a4a50913df74f0d641a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd4b880bc019409ea95baf79374f37e5",
            "placeholder": "​",
            "style": "IPY_MODEL_c0f179f61de94ce6a51498a81cdf9548",
            "value": "Saving checkpoint shards:   0%"
          }
        },
        "7f6a63d2969d4bb68716ffe89b8a7a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_653c3ecd2d864fd7919cef52297458c3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55f1915f76db429fa9274496ea4b4694",
            "value": 0
          }
        },
        "3ce5bfad0650428f83cc81f3be063b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7a08324df7447ad87dfa724b59a9f54",
            "placeholder": "​",
            "style": "IPY_MODEL_b74df2a377934a47bd1d164a2e182a0a",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "963398858e8f4ac1bfffcd1d39bffb2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4b880bc019409ea95baf79374f37e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f179f61de94ce6a51498a81cdf9548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "653c3ecd2d864fd7919cef52297458c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f1915f76db429fa9274496ea4b4694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7a08324df7447ad87dfa724b59a9f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74df2a377934a47bd1d164a2e182a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}