{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaXdi3s4Bs0r"
      },
      "source": [
        "## Setting up and Dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C0_sLL4A__t3",
        "outputId": "de56f54c-0044-42bf-b3f1-abd619ce9317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "Successfully installed transformers-4.55.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes transformers accelerate\n",
        "!pip install --upgrade transformers\n",
        "!pip install huggingface_hub torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "collapsed": true,
        "id": "KGLyl0CYzV-V",
        "outputId": "ea142dc9-74ef-46cf-dd4d-24fef6e085c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.5\n",
            "    Uninstalling huggingface-hub-0.23.5:\n",
            "      Successfully uninstalled huggingface-hub-0.23.5\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "Successfully installed huggingface-hub-0.34.3 tokenizers-0.21.4 transformers-4.55.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "f3e2010eaf5843179504897eb8045867"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "collapsed": true,
        "id": "dfSetKlfBGSp",
        "outputId": "8c0f6ec9-1b16-489d-952d-f28cd5dabdda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2+cpu\n",
            "    Uninstalling torch-2.2.2+cpu:\n",
            "      Successfully uninstalled torch-2.2.2+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "03a800c875f444e6a84b0be3a8b666d8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install huggingface_hub torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cvB51mZJhoJ"
      },
      "source": [
        "## Downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RclZK4G6BV4g",
        "outputId": "6a9b11ee-1f9a-4df6-88df-ce6da2c1a257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model_conversion\n"
          ]
        }
      ],
      "source": [
        "#loading into the model directory\n",
        "%cd /content/model_conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tLbW1B9N-G19"
      },
      "outputs": [],
      "source": [
        "#importing necessary libaraies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tMuyezojBPqX",
        "outputId": "b70b6ef9-b6de-49af-a47d-b0092852039c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n",
            ".gitattributes: 1.57kB [00:00, 2.08MB/s]\n",
            "Fetching 15 files:   7% 1/15 [00:00<00:03,  3.56it/s]\n",
            "config.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "README.md: 100% 608/608 [00:00<00:00, 3.80MB/s]\n",
            "config.json: 5.35kB [00:00, 5.65MB/s]\n",
            "\n",
            "chat_template.jinja: 1.53kB [00:00, 9.43MB/s]\n",
            "\n",
            "generation_config.json: 100% 215/215 [00:00<00:00, 1.43MB/s]\n",
            "\n",
            "preprocessor_config.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 1.09kB [00:00, 1.22MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 777/777 [00:00<00:00, 3.31MB/s]\n",
            "model.safetensors.index.json: 159kB [00:00, 52.1MB/s]\n",
            "\n",
            "processor_config.json: 100% 98.0/98.0 [00:00<00:00, 362kB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/3.08G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/2.82G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 1.20MB [00:00, 309MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 10.5M/4.98G [00:00<04:31, 18.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 10.5M/3.08G [00:00<03:27, 14.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 10.5M/2.82G [00:00<03:33, 13.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 21.0M/4.98G [00:00<03:49, 21.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 21.0M/3.08G [00:01<02:41, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 21.0M/2.82G [00:01<02:37, 17.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 31.5M/4.98G [00:01<03:36, 22.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 31.5M/3.08G [00:01<02:25, 20.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 31.5M/2.82G [00:01<02:18, 20.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 41.9M/4.98G [00:01<03:28, 23.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 41.9M/3.08G [00:02<02:19, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   1% 41.9M/2.82G [00:02<02:09, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 52.4M/4.98G [00:02<03:27, 23.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 52.4M/3.08G [00:02<02:15, 22.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 52.4M/2.82G [00:02<02:05, 22.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 62.9M/4.98G [00:02<03:24, 24.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 62.9M/3.08G [00:02<02:12, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 62.9M/2.82G [00:03<02:01, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 73.4M/4.98G [00:03<03:22, 24.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 73.4M/3.08G [00:03<02:10, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 73.4M/2.82G [00:03<01:59, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 83.9M/4.98G [00:03<03:20, 24.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 83.9M/3.08G [00:03<02:09, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 83.9M/2.82G [00:03<01:58, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 94.4M/4.98G [00:03<03:19, 24.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 94.4M/3.08G [00:04<02:08, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 94.4M/2.82G [00:04<01:56, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 105M/4.98G [00:04<03:18, 24.6MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 105M/3.08G [00:04<02:07, 23.3MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 115M/4.98G [00:04<03:16, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   4% 105M/2.82G [00:04<01:55, 23.4MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   4% 115M/3.08G [00:05<02:06, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 126M/4.98G [00:05<03:16, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   4% 115M/2.82G [00:05<01:55, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   4% 126M/3.08G [00:05<02:09, 22.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 136M/4.98G [00:05<03:15, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   4% 126M/2.82G [00:05<01:54, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   4% 136M/3.08G [00:06<02:06, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 136M/2.82G [00:06<01:53, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 147M/4.98G [00:06<03:42, 21.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 147M/2.82G [00:06<01:53, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 157M/4.98G [00:06<03:33, 22.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 147M/3.08G [00:06<02:23, 20.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 157M/2.82G [00:06<01:52, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 168M/4.98G [00:07<03:27, 23.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 157M/3.08G [00:07<02:17, 21.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 168M/2.82G [00:07<01:52, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 178M/4.98G [00:07<03:23, 23.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 168M/3.08G [00:07<02:12, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 178M/2.82G [00:07<01:51, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 189M/4.98G [00:07<03:20, 23.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   6% 178M/3.08G [00:08<02:09, 22.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 189M/2.82G [00:08<01:51, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 199M/4.98G [00:08<03:17, 24.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   6% 189M/3.08G [00:08<02:07, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 199M/2.82G [00:08<01:50, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 210M/4.98G [00:08<03:16, 24.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   6% 199M/3.08G [00:08<02:05, 22.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 220M/4.98G [00:09<03:14, 24.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   7% 210M/2.82G [00:09<01:50, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 210M/3.08G [00:09<02:04, 23.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 231M/4.98G [00:09<03:13, 24.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 220M/2.82G [00:09<01:50, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 220M/3.08G [00:09<02:02, 23.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 241M/4.98G [00:10<03:12, 24.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 231M/2.82G [00:10<01:49, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 231M/3.08G [00:10<02:02, 23.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 252M/4.98G [00:10<03:11, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 241M/2.82G [00:10<01:49, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 241M/3.08G [00:10<02:01, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 262M/4.98G [00:10<03:10, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 252M/2.82G [00:10<01:48, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 252M/3.08G [00:11<02:00, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 273M/4.98G [00:11<03:10, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   9% 262M/2.82G [00:11<01:48, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 262M/3.08G [00:11<01:59, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 283M/4.98G [00:11<03:09, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 273M/2.82G [00:11<01:47, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 273M/3.08G [00:12<01:59, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 294M/4.98G [00:12<03:09, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 283M/2.82G [00:12<01:47, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 283M/3.08G [00:12<01:58, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 304M/4.98G [00:12<03:08, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 294M/2.82G [00:12<01:46, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  10% 294M/3.08G [00:12<01:58, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 315M/4.98G [00:13<03:08, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  11% 304M/2.82G [00:13<01:46, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  10% 304M/3.08G [00:13<01:57, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 325M/4.98G [00:13<03:09, 24.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  11% 315M/2.82G [00:13<01:46, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  10% 315M/3.08G [00:13<01:57, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 336M/4.98G [00:13<03:07, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 325M/2.82G [00:14<01:45, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 346M/4.98G [00:14<03:06, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 325M/3.08G [00:14<01:57, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 336M/2.82G [00:14<01:45, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 357M/4.98G [00:14<03:06, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 336M/3.08G [00:14<01:56, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  12% 346M/2.82G [00:14<01:44, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 367M/4.98G [00:15<03:05, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 346M/3.08G [00:15<01:56, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 357M/2.82G [00:15<01:44, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 377M/4.98G [00:15<03:05, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 357M/3.08G [00:15<01:55, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 388M/4.98G [00:16<03:05, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 367M/3.08G [00:16<01:55, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 367M/2.82G [00:16<01:57, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 398M/4.98G [00:16<03:05, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 377M/3.08G [00:16<01:54, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 377M/2.82G [00:16<01:53, 21.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 409M/4.98G [00:16<03:04, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  13% 388M/3.08G [00:16<01:54, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 388M/2.82G [00:16<01:49, 22.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 419M/4.98G [00:17<03:04, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  13% 398M/3.08G [00:17<01:53, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 398M/2.82G [00:17<01:47, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 430M/4.98G [00:17<03:03, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  13% 409M/3.08G [00:17<01:53, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 409M/2.82G [00:17<01:45, 22.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 440M/4.98G [00:18<03:03, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 419M/3.08G [00:18<01:53, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  15% 419M/2.82G [00:18<01:44, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 451M/4.98G [00:18<03:02, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 430M/3.08G [00:18<01:52, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  15% 430M/2.82G [00:18<01:43, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 461M/4.98G [00:18<03:02, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 440M/3.08G [00:19<01:52, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 440M/2.82G [00:19<01:42, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 472M/4.98G [00:19<03:01, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 451M/2.82G [00:19<01:40, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 451M/3.08G [00:19<01:52, 23.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 482M/4.98G [00:19<03:01, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 461M/2.82G [00:20<01:40, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 461M/3.08G [00:20<01:51, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 493M/4.98G [00:20<03:00, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  17% 472M/2.82G [00:20<01:39, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 472M/3.08G [00:20<01:50, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 503M/4.98G [00:20<03:00, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  17% 482M/2.82G [00:20<01:39, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 482M/3.08G [00:21<01:50, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 514M/4.98G [00:21<03:00, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  17% 493M/2.82G [00:21<01:38, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 493M/3.08G [00:21<01:49, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 524M/4.98G [00:21<02:59, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 503M/2.82G [00:21<01:38, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 503M/3.08G [00:21<01:49, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 535M/4.98G [00:21<02:59, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 514M/2.82G [00:22<01:37, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  17% 514M/3.08G [00:22<01:50, 23.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 545M/4.98G [00:22<02:59, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 524M/2.82G [00:22<01:37, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 556M/4.98G [00:22<02:59, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  17% 524M/3.08G [00:22<01:48, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 566M/4.98G [00:23<02:57, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 535M/2.82G [00:23<01:36, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  17% 535M/3.08G [00:23<01:47, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 577M/4.98G [00:23<02:57, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 545M/2.82G [00:23<01:36, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 545M/3.08G [00:23<01:47, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 587M/4.98G [00:24<02:56, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  20% 556M/2.82G [00:24<01:35, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 556M/3.08G [00:24<01:47, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 598M/4.98G [00:24<02:57, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  20% 566M/2.82G [00:24<01:35, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 566M/3.08G [00:24<02:02, 20.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 608M/4.98G [00:24<02:56, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  20% 577M/2.82G [00:24<01:34, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 577M/3.08G [00:25<01:57, 21.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 619M/4.98G [00:25<02:55, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 587M/2.82G [00:25<01:34, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 587M/3.08G [00:25<01:53, 21.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 629M/4.98G [00:25<02:55, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 598M/2.82G [00:25<01:34, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 598M/3.08G [00:26<01:50, 22.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 640M/4.98G [00:26<02:55, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 608M/2.82G [00:26<01:33, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 608M/3.08G [00:26<01:48, 22.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 650M/4.98G [00:26<02:54, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 619M/2.82G [00:26<01:33, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 661M/4.98G [00:27<02:54, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 619M/3.08G [00:27<01:47, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 629M/2.82G [00:27<01:32, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 671M/4.98G [00:27<02:53, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 629M/3.08G [00:27<01:45, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 640M/2.82G [00:27<01:32, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 682M/4.98G [00:27<02:53, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  21% 640M/3.08G [00:27<01:44, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 650M/2.82G [00:28<01:31, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 692M/4.98G [00:28<02:52, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 661M/2.82G [00:28<01:31, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  21% 650M/3.08G [00:28<01:58, 20.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 703M/4.98G [00:28<02:52, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 671M/2.82G [00:28<01:31, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  21% 661M/3.08G [00:29<01:53, 21.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 713M/4.98G [00:29<02:51, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 682M/2.82G [00:29<01:30, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 671M/3.08G [00:29<01:49, 21.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 724M/4.98G [00:29<02:51, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 692M/2.82G [00:29<01:30, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 682M/3.08G [00:29<01:46, 22.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 734M/4.98G [00:29<02:51, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 703M/2.82G [00:30<01:29, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 692M/3.08G [00:30<01:45, 22.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 744M/4.98G [00:30<02:51, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  25% 713M/2.82G [00:30<01:29, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 703M/3.08G [00:30<01:43, 23.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 755M/4.98G [00:30<02:50, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 724M/2.82G [00:31<01:28, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 765M/4.98G [00:31<02:49, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 713M/3.08G [00:31<01:42, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 734M/2.82G [00:31<01:28, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 776M/4.98G [00:31<02:49, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 724M/3.08G [00:31<01:41, 23.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 786M/4.98G [00:32<02:49, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 744M/2.82G [00:32<01:27, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 734M/3.08G [00:32<01:40, 23.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 797M/4.98G [00:32<02:48, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 755M/2.82G [00:32<01:27, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 744M/3.08G [00:32<01:39, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 807M/4.98G [00:32<02:48, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 765M/2.82G [00:32<01:27, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  25% 755M/3.08G [00:33<01:39, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 818M/4.98G [00:33<02:47, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 776M/2.82G [00:33<01:26, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  25% 765M/3.08G [00:33<01:38, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 828M/4.98G [00:33<02:47, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 786M/2.82G [00:33<01:26, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  25% 776M/3.08G [00:33<01:38, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 839M/4.98G [00:34<02:47, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  28% 797M/2.82G [00:34<01:25, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 786M/3.08G [00:34<01:37, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 849M/4.98G [00:34<02:46, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 807M/2.82G [00:34<01:25, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 797M/3.08G [00:34<01:37, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 860M/4.98G [00:35<02:46, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 818M/2.82G [00:35<01:24, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 807M/3.08G [00:35<01:36, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 870M/4.98G [00:35<02:45, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 828M/2.82G [00:35<01:24, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 818M/3.08G [00:35<01:36, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 881M/4.98G [00:35<02:45, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 828M/3.08G [00:36<01:35, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  30% 839M/2.82G [00:36<01:33, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 891M/4.98G [00:36<02:44, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 839M/3.08G [00:36<01:35, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  30% 849M/2.82G [00:36<01:30, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 902M/4.98G [00:36<02:44, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 849M/3.08G [00:37<01:34, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  30% 860M/2.82G [00:37<01:27, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 912M/4.98G [00:37<02:44, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 860M/3.08G [00:37<01:34, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 923M/4.98G [00:37<02:43, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  31% 870M/2.82G [00:37<01:26, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 870M/3.08G [00:37<01:33, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 933M/4.98G [00:38<02:43, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  31% 881M/2.82G [00:37<01:24, 22.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  29% 881M/3.08G [00:38<01:33, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 944M/4.98G [00:38<02:42, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 891M/2.82G [00:38<01:23, 23.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  29% 891M/3.08G [00:38<01:32, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 954M/4.98G [00:38<02:42, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 902M/2.82G [00:38<01:22, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  29% 902M/3.08G [00:39<01:32, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 965M/4.98G [00:39<02:42, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 912M/2.82G [00:39<01:34, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 912M/3.08G [00:39<01:31, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 975M/4.98G [00:39<02:59, 22.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  33% 923M/2.82G [00:40<01:29, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 923M/3.08G [00:40<01:31, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 986M/4.98G [00:40<02:54, 22.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  33% 933M/2.82G [00:40<01:26, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 933M/3.08G [00:40<01:31, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 996M/4.98G [00:40<02:50, 23.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  33% 944M/2.82G [00:40<01:24, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 944M/3.08G [00:41<01:30, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.98G [00:41<02:47, 23.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 954M/2.82G [00:41<01:22, 22.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 954M/3.08G [00:41<01:30, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.02G/4.98G [00:41<02:45, 23.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 965M/2.82G [00:41<01:20, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 965M/3.08G [00:41<01:29, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.03G/4.98G [00:42<02:43, 24.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 975M/3.08G [00:42<01:29, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.04G/4.98G [00:42<02:42, 24.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 975M/2.82G [00:42<01:30, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 986M/3.08G [00:42<01:28, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.05G/4.98G [00:42<02:40, 24.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 986M/2.82G [00:42<01:26, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.06G/4.98G [00:43<02:39, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 996M/3.08G [00:43<01:28, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 996M/2.82G [00:43<01:23, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.07G/4.98G [00:43<02:38, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  33% 1.01G/3.08G [00:43<01:28, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  36% 1.01G/2.82G [00:43<01:20, 22.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.08G/4.98G [00:44<02:38, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  33% 1.02G/3.08G [00:44<01:27, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  36% 1.02G/2.82G [00:44<01:19, 22.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.09G/4.98G [00:44<02:37, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  33% 1.03G/3.08G [00:44<01:27, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  36% 1.03G/2.82G [00:44<01:17, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.10G/4.98G [00:44<02:36, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.04G/3.08G [00:45<01:26, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 1.04G/2.82G [00:45<01:16, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.11G/4.98G [00:45<02:36, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.05G/3.08G [00:45<01:26, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 1.05G/2.82G [00:45<01:16, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.12G/4.98G [00:45<02:35, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.06G/3.08G [00:45<01:25, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 1.06G/2.82G [00:45<01:15, 23.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.13G/4.98G [00:46<02:35, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.07G/3.08G [00:46<01:25, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 1.07G/2.82G [00:46<01:14, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.98G [00:46<02:34, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.08G/3.08G [00:46<01:24, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  38% 1.08G/2.82G [00:46<01:13, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.15G/4.98G [00:47<02:34, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.09G/3.08G [00:47<01:24, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.09G/2.82G [00:47<01:13, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.16G/4.98G [00:47<02:33, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.10G/3.08G [00:47<01:23, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.10G/2.82G [00:47<01:12, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.17G/4.98G [00:47<02:33, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.11G/3.08G [00:48<01:23, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.11G/2.82G [00:48<01:12, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.18G/4.98G [00:48<02:33, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.12G/3.08G [00:48<01:23, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 1.12G/2.82G [00:48<01:11, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.20G/4.98G [00:48<02:32, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  37% 1.13G/3.08G [00:49<01:22, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 1.13G/2.82G [00:49<01:11, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.98G [00:49<02:32, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  37% 1.14G/3.08G [00:49<01:22, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  41% 1.14G/2.82G [00:49<01:10, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.22G/4.98G [00:49<02:31, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  37% 1.15G/3.08G [00:49<01:21, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  41% 1.15G/2.82G [00:49<01:10, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.23G/4.98G [00:50<02:31, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.16G/3.08G [00:50<01:21, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  41% 1.16G/2.82G [00:50<01:09, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.24G/4.98G [00:50<02:31, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.17G/3.08G [00:50<01:20, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.25G/4.98G [00:50<02:30, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  42% 1.17G/2.82G [00:50<01:09, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.26G/4.98G [00:51<02:30, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.18G/3.08G [00:51<01:20, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  42% 1.18G/2.82G [00:51<01:09, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.27G/4.98G [00:51<02:29, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.20G/3.08G [00:51<01:19, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  42% 1.20G/2.82G [00:51<01:08, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.98G [00:52<02:29, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.21G/3.08G [00:52<01:20, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.21G/2.82G [00:52<01:09, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.29G/4.98G [00:52<02:28, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 1.22G/3.08G [00:52<01:18, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.22G/2.82G [00:52<01:07, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.30G/4.98G [00:53<02:28, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 1.23G/3.08G [00:53<01:18, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.23G/2.82G [00:53<01:07, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.31G/4.98G [00:53<02:28, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 1.24G/3.08G [00:53<01:18, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  44% 1.24G/2.82G [00:53<01:06, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.32G/4.98G [00:53<02:27, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  41% 1.25G/3.08G [00:53<01:17, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  44% 1.25G/2.82G [00:53<01:06, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.33G/4.98G [00:54<02:27, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  41% 1.26G/3.08G [00:54<01:17, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 1.26G/2.82G [00:54<01:05, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.98G [00:54<02:26, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  41% 1.27G/3.08G [00:54<01:16, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 1.27G/2.82G [00:54<01:05, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.35G/4.98G [00:55<02:26, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 1.28G/3.08G [00:55<01:16, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 1.28G/2.82G [00:55<01:05, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.36G/4.98G [00:57<05:27, 11.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 1.29G/3.08G [00:57<02:34, 11.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  46% 1.29G/2.82G [00:57<02:12, 11.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.38G/4.98G [00:57<03:05, 19.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 1.32G/3.08G [00:57<01:11, 24.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  46% 1.31G/2.82G [00:57<01:15, 19.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.98G [00:57<02:01, 29.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  47% 1.32G/2.82G [00:57<01:00, 24.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 1.33G/3.08G [00:57<01:06, 26.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.42G/4.98G [00:57<01:49, 32.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  47% 1.33G/2.82G [00:57<00:51, 29.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.43G/4.98G [00:58<01:55, 30.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 1.34G/3.08G [00:58<01:07, 25.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 1.34G/2.82G [00:58<00:53, 27.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.44G/4.98G [00:58<02:02, 28.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 1.35G/3.08G [00:58<01:08, 25.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 1.35G/2.82G [00:58<00:55, 26.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.45G/4.98G [00:59<02:07, 27.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 1.36G/3.08G [00:59<01:09, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 1.36G/2.82G [00:59<01:04, 22.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.46G/4.98G [00:59<02:11, 26.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  45% 1.37G/3.08G [00:59<01:10, 24.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  49% 1.37G/2.82G [00:59<01:03, 22.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.47G/4.98G [00:59<02:13, 26.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  45% 1.38G/3.08G [00:59<01:10, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  49% 1.38G/2.82G [01:00<01:02, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.98G [01:00<02:15, 25.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  45% 1.39G/3.08G [01:00<01:10, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  49% 1.39G/2.82G [01:00<01:01, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.49G/4.98G [01:00<02:16, 25.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 1.41G/3.08G [01:00<01:10, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 1.41G/2.82G [01:01<01:00, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.50G/4.98G [01:01<02:17, 25.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 1.42G/3.08G [01:01<01:09, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 1.42G/2.82G [01:01<00:59, 23.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.51G/4.98G [01:01<02:18, 25.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 1.43G/3.08G [01:01<01:09, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 1.43G/2.82G [01:01<00:59, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.52G/4.98G [01:01<02:18, 24.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 1.44G/3.08G [01:02<01:09, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.53G/4.98G [01:02<02:17, 25.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 1.44G/2.82G [01:02<00:58, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 1.45G/3.08G [01:02<01:09, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.98G [01:02<02:17, 24.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 1.45G/2.82G [01:02<00:58, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 1.46G/3.08G [01:03<01:08, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.55G/4.98G [01:03<02:17, 24.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  52% 1.46G/2.82G [01:03<00:57, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 1.47G/3.08G [01:03<01:08, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.56G/4.98G [01:03<02:17, 24.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  52% 1.47G/2.82G [01:03<00:57, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 1.48G/3.08G [01:03<01:07, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.57G/4.98G [01:04<02:17, 24.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  52% 1.48G/2.82G [01:04<00:56, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 1.49G/3.08G [01:04<01:07, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.58G/4.98G [01:04<02:16, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 1.49G/2.82G [01:04<00:56, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  49% 1.50G/3.08G [01:04<01:06, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.59G/4.98G [01:04<02:20, 24.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 1.50G/2.82G [01:05<00:55, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  49% 1.51G/3.08G [01:05<01:06, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.60G/4.98G [01:05<02:19, 24.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  54% 1.51G/2.82G [01:05<00:55, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  49% 1.52G/3.08G [01:05<01:06, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.98G [01:05<02:17, 24.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  54% 1.52G/2.82G [01:05<00:54, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 1.53G/3.08G [01:06<01:05, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.63G/4.98G [01:06<02:16, 24.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  54% 1.53G/2.82G [01:06<00:54, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 1.54G/3.08G [01:06<01:05, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.64G/4.98G [01:06<02:19, 24.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 1.54G/2.82G [01:06<00:54, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 1.55G/3.08G [01:07<01:04, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.65G/4.98G [01:07<02:17, 24.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 1.55G/2.82G [01:07<00:53, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 1.56G/3.08G [01:07<01:04, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.66G/4.98G [01:07<02:16, 24.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 1.56G/2.82G [01:07<00:53, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.67G/4.98G [01:07<02:15, 24.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 1.57G/3.08G [01:07<01:03, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 1.57G/2.82G [01:08<00:52, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.98G [01:08<02:14, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 1.58G/3.08G [01:08<01:03, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 1.58G/2.82G [01:08<00:52, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.69G/4.98G [01:08<02:17, 23.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  57% 1.59G/2.82G [01:09<00:51, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  52% 1.59G/3.08G [01:09<01:11, 20.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.70G/4.98G [01:09<02:15, 24.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  52% 1.60G/3.08G [01:09<01:00, 24.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  57% 1.60G/2.82G [01:09<00:51, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.71G/4.98G [01:09<02:14, 24.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  52% 1.61G/3.08G [01:09<01:00, 24.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  57% 1.61G/2.82G [01:09<00:50, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.72G/4.98G [01:10<02:13, 24.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  53% 1.63G/3.08G [01:10<01:00, 24.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 1.63G/2.82G [01:10<00:50, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.73G/4.98G [01:10<02:12, 24.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  53% 1.64G/3.08G [01:10<01:00, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 1.64G/2.82G [01:10<00:50, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.74G/4.98G [01:10<02:11, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 1.65G/3.08G [01:11<01:00, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 1.65G/2.82G [01:11<00:49, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.98G [01:11<02:10, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 1.66G/3.08G [01:11<00:59, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 1.66G/2.82G [01:11<00:49, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.76G/4.98G [01:11<02:10, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 1.67G/3.08G [01:12<00:59, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 1.67G/2.82G [01:12<00:48, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.77G/4.98G [01:12<02:09, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 1.68G/3.08G [01:12<00:59, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 1.68G/2.82G [01:12<00:48, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.78G/4.98G [01:12<02:28, 21.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 1.69G/3.08G [01:12<00:58, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  60% 1.69G/2.82G [01:13<00:47, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.79G/4.98G [01:13<02:23, 22.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 1.70G/3.08G [01:13<00:58, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  60% 1.70G/2.82G [01:13<00:47, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  56% 1.71G/3.08G [01:13<00:58, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 1.71G/2.82G [01:13<00:47, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.80G/4.98G [01:13<02:38, 20.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  56% 1.72G/3.08G [01:14<00:57, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 1.72G/2.82G [01:14<00:46, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.81G/4.98G [01:14<02:29, 21.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  56% 1.73G/3.08G [01:14<00:57, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 1.73G/2.82G [01:14<00:46, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.82G/4.98G [01:15<02:40, 19.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 1.74G/3.08G [01:15<00:56, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  62% 1.74G/2.82G [01:15<00:45, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 1.75G/3.08G [01:15<00:56, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.84G/4.98G [01:15<02:48, 18.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  62% 1.75G/2.82G [01:15<00:45, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 1.76G/3.08G [01:16<00:55, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.85G/4.98G [01:16<02:36, 20.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  62% 1.76G/2.82G [01:16<00:44, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 1.77G/3.08G [01:16<00:55, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 1.77G/2.82G [01:16<00:44, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.86G/4.98G [01:16<02:45, 18.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 1.78G/3.08G [01:16<00:54, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 1.78G/2.82G [01:16<00:43, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.87G/4.98G [01:17<02:33, 20.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 1.79G/3.08G [01:17<00:54, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  64% 1.79G/2.82G [01:17<00:43, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.98G [01:17<02:42, 19.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 1.80G/3.08G [01:17<00:54, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  64% 1.80G/2.82G [01:17<00:42, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.89G/4.98G [01:18<02:31, 20.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 1.81G/3.08G [01:18<00:53, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  64% 1.81G/2.82G [01:18<00:42, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 1.82G/3.08G [01:18<00:53, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 1.82G/2.82G [01:18<00:42, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.90G/4.98G [01:18<02:41, 19.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  60% 1.84G/3.08G [01:19<00:52, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 1.84G/2.82G [01:19<00:41, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.91G/4.98G [01:19<02:30, 20.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  60% 1.85G/3.08G [01:19<00:52, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  65% 1.85G/2.82G [01:19<00:41, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.92G/4.98G [01:19<02:38, 19.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  60% 1.86G/3.08G [01:20<00:51, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 1.86G/2.82G [01:20<00:40, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.93G/4.98G [01:20<02:28, 20.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  61% 1.87G/3.08G [01:20<00:51, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 1.87G/2.82G [01:20<00:40, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.94G/4.98G [01:20<02:20, 21.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  61% 1.88G/3.08G [01:20<00:51, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 1.88G/2.82G [01:20<00:39, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.98G [01:21<02:32, 19.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  61% 1.89G/3.08G [01:21<00:50, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 1.89G/2.82G [01:21<00:39, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.96G/4.98G [01:21<02:23, 21.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 1.90G/3.08G [01:21<00:50, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 1.90G/2.82G [01:21<00:39, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.97G/4.98G [01:22<02:17, 22.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 1.91G/3.08G [01:22<00:49, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 1.91G/2.82G [01:22<00:38, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 1.92G/3.08G [01:22<00:49, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 1.92G/2.82G [01:22<00:38, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.98G/4.98G [01:22<02:28, 20.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 1.93G/3.08G [01:23<00:48, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 1.93G/2.82G [01:23<00:37, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.99G/4.98G [01:23<02:21, 21.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 1.94G/3.08G [01:23<00:48, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  69% 1.94G/2.82G [01:23<00:37, 23.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 2.00G/4.98G [01:23<02:15, 22.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 1.95G/3.08G [01:24<00:47, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  69% 1.95G/2.82G [01:24<00:36, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 2.01G/4.98G [01:24<02:25, 20.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  64% 1.96G/3.08G [01:24<00:47, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  70% 1.96G/2.82G [01:24<00:36, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.02G/4.98G [01:24<02:18, 21.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  64% 1.97G/3.08G [01:24<00:47, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  70% 1.97G/2.82G [01:24<00:35, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.03G/4.98G [01:25<02:12, 22.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  64% 1.98G/3.08G [01:25<00:46, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  70% 1.98G/2.82G [01:25<00:35, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.04G/4.98G [01:25<02:24, 20.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  65% 1.99G/3.08G [01:25<00:46, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 1.99G/2.82G [01:25<00:35, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.06G/4.98G [01:26<02:16, 21.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  65% 2.00G/3.08G [01:26<00:45, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.00G/2.82G [01:26<00:34, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.07G/4.98G [01:26<02:11, 22.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  65% 2.01G/3.08G [01:26<00:45, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.01G/2.82G [01:26<00:34, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.98G [01:27<02:07, 22.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 2.02G/3.08G [01:27<00:44, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 2.02G/2.82G [01:27<00:33, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 2.03G/3.08G [01:27<00:44, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 2.03G/2.82G [01:27<00:33, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.09G/4.98G [01:27<02:19, 20.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 2.04G/3.08G [01:28<00:43, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 2.04G/2.82G [01:28<00:32, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.10G/4.98G [01:28<02:13, 21.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 2.06G/3.08G [01:28<00:43, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.11G/4.98G [01:28<02:08, 22.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  73% 2.06G/2.82G [01:28<00:32, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 2.07G/3.08G [01:28<00:42, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  73% 2.07G/2.82G [01:28<00:31, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.12G/4.98G [01:29<02:20, 20.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 2.08G/3.08G [01:29<00:42, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 2.08G/2.82G [01:29<00:31, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.13G/4.98G [01:29<02:12, 21.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  68% 2.09G/3.08G [01:29<00:42, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 2.09G/2.82G [01:29<00:31, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.14G/4.98G [01:30<02:07, 22.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  68% 2.10G/3.08G [01:30<00:41, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 2.10G/2.82G [01:30<00:30, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/4.98G [01:30<02:18, 20.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  68% 2.11G/3.08G [01:30<00:41, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  75% 2.11G/2.82G [01:30<00:30, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.16G/4.98G [01:31<02:11, 21.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  75% 2.12G/2.82G [01:31<00:29, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  69% 2.12G/3.08G [01:31<00:45, 21.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.17G/4.98G [01:31<02:06, 22.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  75% 2.13G/2.82G [01:31<00:29, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  69% 2.13G/3.08G [01:31<00:43, 21.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/4.98G [01:31<02:02, 22.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 2.14G/2.82G [01:32<00:28, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 2.14G/3.08G [01:32<00:42, 22.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.19G/4.98G [01:32<02:14, 20.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 2.15G/2.82G [01:32<00:28, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 2.15G/3.08G [01:32<00:41, 22.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.20G/4.98G [01:32<02:07, 21.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 2.16G/2.82G [01:32<00:28, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 2.16G/3.08G [01:33<00:40, 22.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.21G/4.98G [01:33<02:02, 22.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 2.17G/2.82G [01:33<00:27, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 2.17G/3.08G [01:33<00:39, 23.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.22G/4.98G [01:33<01:59, 23.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 2.18G/2.82G [01:33<00:27, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 2.18G/3.08G [01:34<00:38, 23.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.23G/4.98G [01:34<01:56, 23.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 2.19G/2.82G [01:34<00:26, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 2.19G/3.08G [01:34<00:38, 23.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.24G/4.98G [01:34<01:54, 23.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 2.20G/2.82G [01:34<00:26, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  72% 2.20G/3.08G [01:34<00:37, 23.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.25G/4.98G [01:35<01:53, 24.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  78% 2.21G/2.82G [01:35<00:25, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  72% 2.21G/3.08G [01:35<00:36, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.26G/4.98G [01:35<01:51, 24.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 2.22G/2.82G [01:35<00:25, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  72% 2.22G/3.08G [01:35<00:36, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 2.23G/2.82G [01:36<00:24, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.98G [01:36<02:04, 21.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 2.23G/3.08G [01:36<00:35, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 2.24G/2.82G [01:36<00:24, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.29G/4.98G [01:36<01:59, 22.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 2.24G/3.08G [01:36<00:35, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.30G/4.98G [01:36<01:56, 23.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 2.25G/2.82G [01:36<00:23, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 2.25G/3.08G [01:37<00:35, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.31G/4.98G [01:37<01:53, 23.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 2.26G/2.82G [01:37<00:23, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 2.26G/3.08G [01:37<00:34, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.32G/4.98G [01:37<01:51, 24.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 2.28G/2.82G [01:37<00:22, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 2.28G/3.08G [01:38<00:34, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.33G/4.98G [01:38<01:49, 24.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 2.29G/2.82G [01:38<00:22, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 2.29G/3.08G [01:38<00:33, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.34G/4.98G [01:38<01:48, 24.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  81% 2.30G/2.82G [01:38<00:22, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 2.30G/3.08G [01:38<00:33, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.98G [01:39<01:47, 24.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 2.31G/2.82G [01:39<00:21, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 2.31G/3.08G [01:39<00:32, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.36G/4.98G [01:39<01:46, 24.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 2.32G/2.82G [01:39<00:21, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 2.32G/3.08G [01:39<00:32, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.37G/4.98G [01:39<01:45, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  83% 2.33G/2.82G [01:40<00:20, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  76% 2.33G/3.08G [01:40<00:31, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.38G/4.98G [01:40<01:45, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  83% 2.34G/2.82G [01:40<00:20, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  76% 2.34G/3.08G [01:40<00:31, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.39G/4.98G [01:40<01:44, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  83% 2.35G/2.82G [01:40<00:19, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  76% 2.35G/3.08G [01:41<00:30, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.40G/4.98G [01:41<01:44, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 2.36G/2.82G [01:41<00:19, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 2.36G/3.08G [01:41<00:30, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.41G/4.98G [01:41<01:43, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 2.37G/2.82G [01:41<00:19, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.98G [01:42<01:43, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 2.37G/3.08G [01:42<00:30, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 2.38G/2.82G [01:42<00:18, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.43G/4.98G [01:42<01:42, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 2.38G/3.08G [01:42<00:29, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 2.39G/2.82G [01:42<00:18, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.44G/4.98G [01:42<01:42, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 2.39G/3.08G [01:42<00:29, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 2.40G/2.82G [01:43<00:17, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.45G/4.98G [01:43<01:41, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 2.40G/3.08G [01:43<00:28, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.46G/4.98G [01:43<01:41, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 2.41G/2.82G [01:43<00:19, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 2.41G/3.08G [01:43<00:28, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.47G/4.98G [01:44<01:41, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  86% 2.42G/2.82G [01:44<00:18, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 2.42G/3.08G [01:44<00:27, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.49G/4.98G [01:44<01:40, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  86% 2.43G/2.82G [01:44<00:17, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 2.43G/3.08G [01:44<00:27, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.50G/4.98G [01:45<01:40, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 2.44G/2.82G [01:45<00:16, 22.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 2.44G/3.08G [01:45<00:26, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.51G/4.98G [01:45<01:39, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 2.45G/2.82G [01:45<00:15, 23.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  80% 2.45G/3.08G [01:45<00:26, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.52G/4.98G [01:45<01:39, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 2.46G/2.82G [01:45<00:15, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  80% 2.46G/3.08G [01:46<00:26, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.53G/4.98G [01:46<01:38, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 2.47G/2.82G [01:46<00:14, 23.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  80% 2.47G/3.08G [01:46<00:25, 23.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.54G/4.98G [01:46<01:38, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 2.49G/2.82G [01:46<00:14, 23.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 2.49G/3.08G [01:46<00:25, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.98G [01:47<01:38, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 2.50G/2.82G [01:47<00:13, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 2.50G/3.08G [01:47<00:24, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.56G/4.98G [01:47<01:37, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  89% 2.51G/2.82G [01:47<00:13, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 2.51G/3.08G [01:47<00:24, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.57G/4.98G [01:47<01:37, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  89% 2.52G/2.82G [01:48<00:12, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 2.52G/3.08G [01:48<00:23, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.58G/4.98G [01:48<01:36, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 2.53G/2.82G [01:48<00:12, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 2.53G/3.08G [01:48<00:23, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.59G/4.98G [01:48<01:36, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 2.54G/2.82G [01:49<00:12, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 2.54G/3.08G [01:49<00:22, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.60G/4.98G [01:49<01:36, 24.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 2.55G/2.82G [01:49<00:11, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 2.55G/3.08G [01:49<00:22, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.61G/4.98G [01:49<01:35, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  91% 2.56G/2.82G [01:49<00:11, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.98G [01:50<01:35, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 2.56G/3.08G [01:50<00:22, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  91% 2.57G/2.82G [01:52<00:25, 9.78MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.63G/4.98G [01:52<03:57, 9.91MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 2.57G/3.08G [01:52<00:52, 9.74MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 2.60G/2.82G [01:52<00:10, 21.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  84% 2.59G/3.08G [01:52<00:28, 17.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.65G/4.98G [01:52<02:14, 17.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 2.62G/2.82G [01:52<00:06, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  85% 2.61G/3.08G [01:52<00:17, 26.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.67G/4.98G [01:52<01:27, 26.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.98G [01:53<01:13, 31.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 2.63G/3.08G [01:53<00:12, 34.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/4.98G [01:53<01:03, 35.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.71G/4.98G [01:53<01:10, 32.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 2.64G/3.08G [01:53<00:13, 31.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  94% 2.64G/2.82G [01:53<00:06, 27.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.72G/4.98G [01:54<01:15, 29.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 2.65G/3.08G [01:54<00:14, 29.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  94% 2.65G/2.82G [01:54<00:06, 26.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.73G/4.98G [01:54<01:19, 28.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  87% 2.66G/3.08G [01:54<00:15, 27.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  94% 2.66G/2.82G [01:54<00:06, 26.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.74G/4.98G [01:54<01:22, 27.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  87% 2.67G/3.08G [01:54<00:15, 26.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 2.67G/2.82G [01:55<00:05, 25.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.98G [01:55<01:24, 26.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  87% 2.68G/3.08G [01:55<00:15, 25.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 2.68G/2.82G [01:55<00:05, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.76G/4.98G [01:55<01:25, 26.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  88% 2.69G/3.08G [01:55<00:15, 25.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 2.69G/2.82G [01:55<00:05, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.77G/4.98G [01:56<01:30, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  88% 2.71G/3.08G [01:56<00:17, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 2.71G/2.82G [01:56<00:05, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.78G/4.98G [01:56<01:29, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  88% 2.72G/3.08G [01:56<00:14, 25.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 2.72G/2.82G [01:56<00:04, 24.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.79G/4.98G [01:57<01:28, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 2.73G/3.08G [01:57<00:14, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 2.73G/2.82G [01:57<00:03, 23.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.80G/4.98G [01:57<01:29, 24.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 2.74G/3.08G [01:57<00:13, 24.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 2.74G/2.82G [01:57<00:03, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.81G/4.98G [01:57<01:28, 24.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 2.75G/3.08G [01:58<00:13, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 2.75G/2.82G [01:58<00:03, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.98G [01:58<01:27, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  90% 2.76G/3.08G [01:58<00:13, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 2.76G/2.82G [01:58<00:02, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.83G/4.98G [01:58<01:27, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  90% 2.77G/3.08G [01:59<00:12, 23.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 2.77G/2.82G [01:59<00:02, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.84G/4.98G [01:59<01:26, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  90% 2.78G/3.08G [01:59<00:12, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  99% 2.78G/2.82G [01:59<00:01, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.85G/4.98G [01:59<01:26, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 2.79G/3.08G [01:59<00:12, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  99% 2.79G/2.82G [01:59<00:01, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.86G/4.98G [02:00<01:25, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 2.80G/3.08G [02:00<00:11, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  99% 2.80G/2.82G [02:00<00:00, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.87G/4.98G [02:00<01:25, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 2.81G/3.08G [02:00<00:11, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 2.81G/2.82G [02:00<00:00, 23.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.88G/4.98G [02:00<01:24, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 2.82G/3.08G [02:01<00:10, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.98G [02:01<01:24, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 2.82G/2.82G [02:01<00:00, 23.2MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 2.83G/3.08G [02:01<00:10, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.90G/4.98G [02:01<01:23, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 2.84G/3.08G [02:02<00:09, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.92G/4.98G [02:02<01:23, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 2.85G/3.08G [02:02<00:09, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.93G/4.98G [02:02<01:22, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 2.86G/3.08G [02:03<00:09, 23.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.94G/4.98G [02:03<01:22, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.95G/4.98G [02:03<01:22, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 2.87G/3.08G [02:03<00:08, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.96G/4.98G [02:03<01:21, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  94% 2.88G/3.08G [02:03<00:08, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.97G/4.98G [02:04<01:21, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  94% 2.89G/3.08G [02:04<00:07, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.98G/4.98G [02:04<01:20, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  94% 2.90G/3.08G [02:04<00:07, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.99G/4.98G [02:05<01:20, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 2.92G/3.08G [02:05<00:06, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 3.00G/4.98G [02:05<01:19, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 2.93G/3.08G [02:05<00:06, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 3.01G/4.98G [02:06<01:19, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 2.94G/3.08G [02:06<00:06, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.98G [02:06<01:19, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  96% 2.95G/3.08G [02:06<00:05, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.03G/4.98G [02:06<01:18, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  96% 2.96G/3.08G [02:07<00:05, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.04G/4.98G [02:07<01:18, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  96% 2.97G/3.08G [02:07<00:04, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.05G/4.98G [02:07<01:17, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 2.98G/3.08G [02:07<00:04, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.06G/4.98G [02:08<01:17, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 2.99G/3.08G [02:08<00:03, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.07G/4.98G [02:08<01:16, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 3.00G/3.08G [02:08<00:03, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.08G/4.98G [02:08<01:16, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  98% 3.01G/3.08G [02:09<00:02, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.98G [02:09<01:16, 24.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  98% 3.02G/3.08G [02:09<00:02, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.10G/4.98G [02:09<01:15, 24.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  98% 3.03G/3.08G [02:10<00:01, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.11G/4.98G [02:10<01:15, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  99% 3.04G/3.08G [02:10<00:01, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.12G/4.98G [02:10<01:14, 24.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  99% 3.05G/3.08G [02:11<00:01, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.14G/4.98G [02:11<01:14, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 3.06G/3.08G [02:11<00:00, 23.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.15G/4.98G [02:11<01:14, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.16G/4.98G [02:11<01:13, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 3.07G/3.08G [02:11<00:00, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 3.08G/3.08G [02:12<00:00, 23.3MB/s]\n",
            "Fetching 15 files:  40% 6/15 [02:12<03:29, 23.27s/it]\n",
            "model-00002-of-00003.safetensors:  64% 3.17G/4.98G [02:12<01:13, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.18G/4.98G [02:12<01:12, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.19G/4.98G [02:13<01:12, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.20G/4.98G [02:13<01:11, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.21G/4.98G [02:14<01:11, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.98G [02:14<01:11, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.23G/4.98G [02:14<01:10, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.24G/4.98G [02:15<01:10, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.25G/4.98G [02:15<01:09, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.26G/4.98G [02:16<01:09, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.27G/4.98G [02:16<01:08, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.28G/4.98G [02:17<01:08, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.98G [02:17<01:12, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.30G/4.98G [02:17<01:10, 23.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.31G/4.98G [02:18<01:09, 24.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.32G/4.98G [02:18<01:08, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.33G/4.98G [02:19<01:07, 24.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.34G/4.98G [02:19<01:06, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.36G/4.98G [02:20<01:06, 24.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.37G/4.98G [02:20<01:05, 24.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.38G/4.98G [02:20<01:05, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.39G/4.98G [02:21<01:04, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.40G/4.98G [02:21<01:04, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.41G/4.98G [02:22<01:03, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.98G [02:22<01:03, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.43G/4.98G [02:23<01:02, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/4.98G [02:23<01:02, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.45G/4.98G [02:23<01:01, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.46G/4.98G [02:24<01:01, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.47G/4.98G [02:24<01:00, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.48G/4.98G [02:25<01:00, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.98G [02:25<01:00, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.50G/4.98G [02:25<00:59, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.51G/4.98G [02:26<00:59, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.52G/4.98G [02:26<00:58, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.53G/4.98G [02:27<00:58, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.54G/4.98G [02:27<00:57, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.55G/4.98G [02:28<00:57, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.57G/4.98G [02:28<00:57, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.58G/4.98G [02:28<00:56, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.59G/4.98G [02:29<00:56, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.60G/4.98G [02:29<00:55, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.61G/4.98G [02:30<00:55, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.98G [02:30<00:55, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/4.98G [02:31<00:54, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.64G/4.98G [02:31<00:54, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.65G/4.98G [02:32<01:01, 21.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.66G/4.98G [02:32<00:59, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.67G/4.98G [02:33<01:04, 20.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.68G/4.98G [02:33<01:08, 19.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.98G [02:34<01:03, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.70G/4.98G [02:34<01:06, 19.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.71G/4.98G [02:35<01:01, 20.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.72G/4.98G [02:35<00:58, 21.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.73G/4.98G [02:36<00:55, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.74G/4.98G [02:36<01:00, 20.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.75G/4.98G [02:37<00:57, 21.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.98G [02:37<00:54, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.77G/4.98G [02:38<00:52, 23.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.79G/4.98G [02:38<00:50, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.80G/4.98G [02:38<00:49, 23.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.81G/4.98G [02:39<00:48, 24.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.98G [02:39<00:47, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.83G/4.98G [02:40<00:47, 24.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.84G/4.98G [02:40<00:49, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.85G/4.98G [02:41<00:51, 22.0MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.86G/4.98G [02:41<00:49, 22.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.87G/4.98G [02:42<00:47, 23.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.88G/4.98G [02:42<00:46, 23.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/4.98G [02:42<00:45, 24.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.90G/4.98G [02:43<00:44, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.91G/4.98G [02:43<00:43, 24.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.92G/4.98G [02:44<00:43, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.93G/4.98G [02:44<00:42, 24.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.94G/4.98G [02:44<00:42, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.95G/4.98G [02:45<00:41, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.98G [02:45<00:41, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.97G/4.98G [02:46<00:40, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.98G/4.98G [02:46<00:40, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 4.00G/4.98G [02:47<00:39, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 4.01G/4.98G [02:47<00:39, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.02G/4.98G [02:47<00:38, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.98G [02:48<00:38, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.04G/4.98G [02:48<00:38, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.05G/4.98G [02:49<00:37, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.06G/4.98G [02:49<00:37, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.07G/4.98G [02:50<00:36, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.08G/4.98G [02:50<00:36, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.09G/4.98G [02:50<00:35, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.10G/4.98G [02:51<00:35, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.11G/4.98G [02:51<00:35, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.12G/4.98G [02:52<00:34, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.13G/4.98G [02:52<00:34, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.14G/4.98G [02:53<00:33, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.15G/4.98G [02:53<00:33, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.98G [02:53<00:33, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.17G/4.98G [02:54<00:32, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/4.98G [02:54<00:32, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.19G/4.98G [02:55<00:31, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.20G/4.98G [02:55<00:31, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.22G/4.98G [02:55<00:30, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.98G [02:56<00:30, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.24G/4.98G [02:56<00:30, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.25G/4.98G [02:57<00:29, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.26G/4.98G [02:57<00:29, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.27G/4.98G [02:58<00:28, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.28G/4.98G [02:58<00:28, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.29G/4.98G [02:58<00:27, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.30G/4.98G [02:59<00:27, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.31G/4.98G [02:59<00:27, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.32G/4.98G [03:00<00:26, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.33G/4.98G [03:00<00:26, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.34G/4.98G [03:01<00:25, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.35G/4.98G [03:01<00:25, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.98G [03:01<00:24, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.37G/4.98G [03:02<00:24, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.38G/4.98G [03:02<00:24, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.39G/4.98G [03:03<00:23, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.40G/4.98G [03:03<00:23, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.41G/4.98G [03:04<00:22, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.42G/4.98G [03:04<00:22, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.44G/4.98G [03:04<00:22, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.45G/4.98G [03:05<00:21, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.46G/4.98G [03:05<00:21, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.47G/4.98G [03:06<00:20, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.48G/4.98G [03:06<00:20, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.49G/4.98G [03:06<00:19, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.50G/4.98G [03:07<00:19, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.51G/4.98G [03:07<00:19, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.52G/4.98G [03:08<00:18, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.53G/4.98G [03:08<00:18, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.54G/4.98G [03:09<00:17, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.55G/4.98G [03:09<00:17, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.98G [03:09<00:16, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.57G/4.98G [03:10<00:16, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.58G/4.98G [03:10<00:16, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.59G/4.98G [03:11<00:15, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.60G/4.98G [03:11<00:15, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.61G/4.98G [03:12<00:14, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.62G/4.98G [03:12<00:14, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.98G [03:12<00:13, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.65G/4.98G [03:13<00:13, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.66G/4.98G [03:13<00:13, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.67G/4.98G [03:14<00:12, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.68G/4.98G [03:14<00:12, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.69G/4.98G [03:15<00:11, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.70G/4.98G [03:15<00:11, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.71G/4.98G [03:15<00:11, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.72G/4.98G [03:16<00:10, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.73G/4.98G [03:16<00:10, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.74G/4.98G [03:17<00:09, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.75G/4.98G [03:17<00:09, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.98G [03:17<00:08, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.77G/4.98G [03:18<00:08, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.78G/4.98G [03:18<00:08, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.79G/4.98G [03:19<00:07, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.80G/4.98G [03:19<00:07, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.81G/4.98G [03:20<00:06, 24.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.98G [03:20<00:06, 24.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.83G/4.98G [03:20<00:06, 24.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.84G/4.98G [03:21<00:05, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.85G/4.98G [03:21<00:05, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.87G/4.98G [03:22<00:05, 22.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.88G/4.98G [03:22<00:04, 23.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.89G/4.98G [03:23<00:03, 23.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.90G/4.98G [03:23<00:03, 24.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.91G/4.98G [03:24<00:03, 24.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.92G/4.98G [03:24<00:02, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.93G/4.98G [03:24<00:02, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.94G/4.98G [03:25<00:01, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.95G/4.98G [03:25<00:01, 24.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.98G [03:26<00:00, 24.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.97G/4.98G [03:26<00:00, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.98G/4.98G [03:27<00:00, 24.1MB/s]\n",
            "Fetching 15 files: 100% 15/15 [03:27<00:00, 13.84s/it]\n"
          ]
        }
      ],
      "source": [
        "# downloading the model\n",
        "!python download.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VHsh_lkN-7C5",
        "outputId": "8d954a06-e1fc-4d59-f4b5-c4b3ef077380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 11G\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Aug  5 23:36 .\n",
            "4.0K drwxr-xr-x 4 root root 4.0K Aug  5 23:36 ..\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Aug  5 23:17 .cache\n",
            "4.0K -rw-r--r-- 1 root root 1.5K Aug  5 23:33 chat_template.jinja\n",
            "8.0K -rw-r--r-- 1 root root 5.3K Aug  5 23:33 config.json\n",
            "4.0K -rw-r--r-- 1 root root  215 Aug  5 23:33 generation_config.json\n",
            "4.0K -rw-r--r-- 1 root root 1.6K Aug  5 23:33 .gitattributes\n",
            "4.0K drwxr-xr-x 3 root root 4.0K Aug  5 23:33 .huggingface\n",
            "2.9G -rw-r--r-- 1 root root 2.9G Aug  5 23:35 model-00001-of-00003.safetensors\n",
            "4.7G -rw-r--r-- 1 root root 4.7G Aug  5 23:36 model-00002-of-00003.safetensors\n",
            "2.7G -rw-r--r-- 1 root root 2.7G Aug  5 23:35 model-00003-of-00003.safetensors\n",
            "156K -rw-r--r-- 1 root root 156K Aug  5 23:33 model.safetensors.index.json\n",
            "4.0K -rw-r--r-- 1 root root 1.1K Aug  5 23:33 preprocessor_config.json\n",
            "4.0K -rw-r--r-- 1 root root   98 Aug  5 23:33 processor_config.json\n",
            "4.0K -rw-r--r-- 1 root root  608 Aug  5 23:33 README.md\n",
            "4.0K -rw-r--r-- 1 root root  777 Aug  5 23:33 special_tokens_map.json\n",
            "1.2M -rw-r--r-- 1 root root 1.2M Aug  5 23:33 tokenizer_config.json\n",
            " 32M -rw-r--r-- 1 root root  32M Aug  5 23:17 tokenizer.json\n",
            "4.5M -rw-r--r-- 1 root root 4.5M Aug  5 23:17 tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "# verifying the model is downloaded\n",
        "!ls -lash vicuna-hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRRtzSdqCoYr"
      },
      "source": [
        "## Converting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ExuxTNb2-7Fj",
        "outputId": "e17ef6e9-1dc4-41a0-8ec2-0d8aaf73eb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 58423, done.\u001b[K\n",
            "remote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (156/156), done.\u001b[K\n",
            "remote: Total 58423 (delta 90), reused 25 (delta 25), pack-reused 58242 (from 3)\u001b[K\n",
            "Receiving objects: 100% (58423/58423), 141.91 MiB | 14.70 MiB/s, done.\n",
            "Resolving deltas: 100% (42225/42225), done.\n"
          ]
        }
      ],
      "source": [
        "#cloning the llama.cpp repo\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "O5MzmQjA-7I8",
        "outputId": "09e51942-2f35-41f5-e33a-f065457d3cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Collecting numpy~=1.26.4 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.54.0)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4))\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting mistral-common>=1.8.3 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1))\n",
            "  Downloading mistral_common-1.8.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting torch~=2.2.1 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp~=3.9.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 1))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest~=8.3.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 2))\n",
            "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting huggingface_hub~=0.23.2 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 3))\n",
            "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n",
            "Collecting openai~=1.55.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 6))\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pandas~=2.2.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 7))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 8))\n",
            "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n",
            "Collecting wget~=3.2 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 10))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typer~=0.15.1 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 11))\n",
            "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.18.0)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers<5.0.0,>=4.45.1 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3))\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (4.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (11.3.0)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1))\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (6.6.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.20.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2025.7.14)\n",
            "Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1)) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common>=1.8.3->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 1))\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (0.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
            "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=56ade34cdab3992a89828f02fd612cc564bd17079820cd7f30218d4d4951466e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, pytest, pycountry, protobuf, prometheus-client, numpy, click, torch, pandas, huggingface_hub, gguf, aiohttp, typer, tokenizers, pydantic-extra-types, openai, transformers, mistral-common\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.4.1\n",
            "    Uninstalling pytest-8.4.1:\n",
            "      Successfully uninstalled pytest-8.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus_client 0.22.1\n",
            "    Uninstalling prometheus_client-0.22.1:\n",
            "      Successfully uninstalled prometheus_client-0.22.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.34.1\n",
            "    Uninstalling huggingface-hub-0.34.1:\n",
            "      Successfully uninstalled huggingface-hub-0.34.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.12.14\n",
            "    Uninstalling aiohttp-3.12.14:\n",
            "      Successfully uninstalled aiohttp-3.12.14\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.16.0\n",
            "    Uninstalling typer-0.16.0:\n",
            "      Successfully uninstalled typer-0.16.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.97.1\n",
            "    Uninstalling openai-1.97.1:\n",
            "      Successfully uninstalled openai-1.97.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "gradio 5.38.2 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "datasets 4.0.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "peft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.5 click-8.1.8 gguf-0.17.1 huggingface_hub-0.23.5 mistral-common-1.8.3 numpy-1.26.4 openai-1.55.3 pandas-2.2.3 prometheus-client-0.20.0 protobuf-4.25.8 pycountry-24.6.1 pydantic-extra-types-2.10.5 pytest-8.3.5 tokenizers-0.20.3 torch-2.2.2+cpu transformers-4.46.3 typer-0.15.4 wget-3.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "27dd104d535541169dfb575c321e643b",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pandas",
                  "tokenizers",
                  "torch",
                  "torchgen",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# installing the required python libraries:\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4wRZHqoPBZ1j",
        "outputId": "3b1c4851-6217-4cd8-96e3-c5eb3eecd047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n",
            "                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}]\n",
            "                             [--bigendian] [--use-temp-file] [--no-lazy]\n",
            "                             [--model-name MODEL_NAME] [--verbose]\n",
            "                             [--split-max-tensors SPLIT_MAX_TENSORS]\n",
            "                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n",
            "                             [--no-tensor-first-split] [--metadata METADATA]\n",
            "                             [--print-supported-models] [--remote] [--mmproj]\n",
            "                             [model]\n",
            "\n",
            "Convert a huggingface model to a GGML compatible file\n",
            "\n",
            "positional arguments:\n",
            "  model                 directory containing model file or huggingface\n",
            "                        repository ID (if --remote)\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --vocab-only          extract only the vocab\n",
            "  --outfile OUTFILE     path to write to; default: based on input. {ftype}\n",
            "                        will be replaced by the outtype.\n",
            "  --outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}\n",
            "                        output format - use f32 for float32, f16 for float16,\n",
            "                        bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for\n",
            "                        ternary, and auto for the highest-fidelity 16-bit\n",
            "                        float type depending on the first loaded tensor type\n",
            "  --bigendian           model is executed on big endian machine\n",
            "  --use-temp-file       use the tempfile library while processing (helpful\n",
            "                        when running out of memory, process killed)\n",
            "  --no-lazy             use more RAM by computing all outputs before writing\n",
            "                        (use in case lazy evaluation is broken)\n",
            "  --model-name MODEL_NAME\n",
            "                        name of the model\n",
            "  --verbose             increase output verbosity\n",
            "  --split-max-tensors SPLIT_MAX_TENSORS\n",
            "                        max tensors in each split\n",
            "  --split-max-size SPLIT_MAX_SIZE\n",
            "                        max size per split N(M|G)\n",
            "  --dry-run             only print out a split plan and exit, without writing\n",
            "                        any new files\n",
            "  --no-tensor-first-split\n",
            "                        do not add tensors to the first split (disabled by\n",
            "                        default)\n",
            "  --metadata METADATA   Specify the path for an authorship metadata override\n",
            "                        file\n",
            "  --print-supported-models\n",
            "                        Print the supported models\n",
            "  --remote              (Experimental) Read safetensors file remotely without\n",
            "                        downloading to disk. Config and tokenizer files will\n",
            "                        still be downloaded. To use this feature, you need to\n",
            "                        specify Hugging Face model repo name instead of a\n",
            "                        local directory. For example:\n",
            "                        'HuggingFaceTB/SmolLM2-1.7B-Instruct'. Note: To access\n",
            "                        gated repo, set HF_TOKEN environment variable to your\n",
            "                        Hugging Face token.\n",
            "  --mmproj              (Experimental) Export multimodal projector (mmproj)\n",
            "                        for vision models. This will only work on some vision\n",
            "                        models. A prefix 'mmproj-' will be added to the output\n",
            "                        file name.\n"
          ]
        }
      ],
      "source": [
        "!python llama.cpp/convert_hf_to_gguf.py -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5JNje9bQBZ7F",
        "outputId": "b7ad16a0-1602-41ec-96b1-b1fefb70e624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: vicuna-hf\n",
            "WARNING:hf-to-gguf:Failed to load model config from vicuna-hf: The checkpoint you are trying to load has model type `gemma3n` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            "WARNING:hf-to-gguf:Failed to load model config from vicuna-hf: The checkpoint you are trying to load has model type `gemma3n` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
            "WARNING:hf-to-gguf:ignore tokens from 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262145: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262146: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262147: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262148: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262149: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262150: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262151: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262152: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262153: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262154: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262155: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262156: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262157: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262158: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262159: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262160: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262161: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262162: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262163: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262164: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262165: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262166: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262167: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262168: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262169: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262170: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262171: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262172: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262173: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262174: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262175: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262176: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262177: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262178: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262179: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262180: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262181: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262182: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262183: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262184: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262185: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262186: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262187: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262188: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262189: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262190: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262191: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262192: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262193: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262194: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262195: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262196: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262197: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262198: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262199: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262200: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262201: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262202: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262203: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262204: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262205: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262206: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262207: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262208: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262209: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262210: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262211: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262212: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262213: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262214: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262215: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262216: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262217: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262218: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262219: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262220: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262221: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262222: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262223: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262224: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262225: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262226: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262227: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262228: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262229: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262230: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262231: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262232: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262233: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262234: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262235: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262236: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262237: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262238: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262239: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262240: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262241: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262242: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262243: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262244: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262245: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262246: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262247: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262248: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262249: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262250: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262251: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262252: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262253: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262254: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262255: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262256: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262257: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262258: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262259: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262260: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262261: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262262: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262263: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262264: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262265: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262266: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262267: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262268: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262269: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262270: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262271: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262272: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262273: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262274: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262275: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262276: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262277: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262278: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262279: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262280: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262281: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262282: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262283: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262284: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262285: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262286: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262287: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262288: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262289: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262290: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262291: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262292: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262293: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262294: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262295: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262296: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262297: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262298: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262299: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262300: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262301: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262302: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262303: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262304: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262305: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262306: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262307: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262308: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262309: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262310: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262311: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262312: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262313: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262314: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262315: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262316: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262317: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262318: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262319: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262320: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262321: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262322: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262323: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262324: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262325: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262326: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262327: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262328: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262329: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262330: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262331: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262332: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262333: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262334: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262335: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262336: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262337: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262338: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262339: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262340: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262341: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262342: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262343: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262344: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262345: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262346: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262347: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262348: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262349: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262350: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262351: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262352: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262353: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262354: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262355: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262356: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262357: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262358: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262359: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262360: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262361: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262362: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262363: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262364: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262365: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262366: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262367: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262368: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262369: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262370: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262371: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262372: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262373: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262374: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262375: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262376: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262377: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262378: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262379: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262380: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262381: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262382: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262383: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262384: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262385: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262386: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262387: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262388: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262389: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262390: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262391: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262392: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262393: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262394: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262395: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262396: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262397: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262398: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262399: id is out of range, max=262143\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 262144}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> Q8_0, shape = {7680, 262144}\n",
            "INFO:hf-to-gguf:blk.0.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.0.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.0.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.0.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.0.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.0.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.1.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.1.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.1.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.1.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.10.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.10.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.10.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.10.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.10.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.11.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.11.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.11.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.11.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.15.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.15.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.15.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.15.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.15.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.16.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.16.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.16.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.16.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.16.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.20.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.20.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.20.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.20.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.20.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.20.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.20.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.21.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.21.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.21.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.21.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.21.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.25.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.25.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.25.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.25.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.25.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.25.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.25.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.25.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.26.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.26.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.26.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.26.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.26.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.5.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.5.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.5.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.5.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.5.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.6.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.6.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.6.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.6.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> Q8_0, shape = {2048, 7680}\n",
            "INFO:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.12.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.12.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.12.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.12.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.12.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.13.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.13.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.13.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.13.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.13.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.14.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.14.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.14.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.14.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.14.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.16.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.17.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.17.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.17.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.17.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.17.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.17.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.18.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.18.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.18.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.18.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.18.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.18.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.18.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.19.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.19.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.19.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.19.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.19.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.19.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.19.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.2.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.2.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.2.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.2.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.2.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.21.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.22.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.22.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.22.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.22.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.22.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.22.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.22.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.23.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.23.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.23.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.23.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.23.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.23.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.23.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.23.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.24.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.24.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.24.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.24.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.24.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.24.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.24.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.24.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.26.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.26.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.27.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.27.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.27.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.27.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.27.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.27.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.27.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.28.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.28.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.28.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.28.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.28.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.28.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.28.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.28.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.29.altup_correct_scale.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.altup_correct_coef.weight,  torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.29.altup_router.weight,        torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.29.altup_predict_coef.weight,  torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.29.altup_router_norm.weight,   torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.laurel_l.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.29.laurel_r.weight,            torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.29.laurel_post_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.29.inp_gate.weight,            torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.29.proj.weight,                torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.post_norm.weight,           torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_k_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_q_norm.weight,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.3.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.3.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.3.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.3.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.3.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.4.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.4.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.4.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.4.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.4.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.7.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.7.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.7.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.7.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.7.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.8.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.8.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.8.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.8.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.8.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.altup_correct_scale.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.altup_correct_coef.weight,   torch.bfloat16 --> F32, shape = {4, 4}\n",
            "INFO:hf-to-gguf:blk.9.altup_router.weight,         torch.bfloat16 --> Q8_0, shape = {2048, 4}\n",
            "INFO:hf-to-gguf:blk.9.altup_predict_coef.weight,   torch.bfloat16 --> F32, shape = {4, 16}\n",
            "INFO:hf-to-gguf:blk.9.altup_router_norm.weight,    torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.laurel_l.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 64}\n",
            "INFO:hf-to-gguf:blk.9.laurel_r.weight,             torch.bfloat16 --> Q8_0, shape = {64, 2048}\n",
            "INFO:hf-to-gguf:blk.9.laurel_post_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> Q8_0, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> Q8_0, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.inp_gate.weight,             torch.bfloat16 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.proj.weight,                 torch.bfloat16 --> Q8_0, shape = {256, 2048}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.post_norm.weight,            torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> Q8_0, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:hf-to-gguf:ignore tokens from 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262144: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262145: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262146: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262147: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262148: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262149: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262150: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262151: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262152: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262153: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262154: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262155: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262156: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262157: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262158: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262159: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262160: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262161: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262162: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262163: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262164: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262165: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262166: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262167: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262168: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262169: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262170: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262171: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262172: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262173: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262174: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262175: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262176: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262177: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262178: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262179: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262180: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262181: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262182: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262183: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262184: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262185: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262186: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262187: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262188: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262189: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262190: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262191: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262192: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262193: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262194: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262195: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262196: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262197: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262198: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262199: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262200: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262201: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262202: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262203: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262204: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262205: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262206: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262207: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262208: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262209: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262210: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262211: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262212: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262213: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262214: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262215: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262216: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262217: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262218: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262219: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262220: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262221: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262222: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262223: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262224: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262225: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262226: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262227: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262228: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262229: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262230: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262231: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262232: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262233: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262234: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262235: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262236: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262237: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262238: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262239: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262240: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262241: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262242: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262243: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262244: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262245: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262246: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262247: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262248: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262249: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262250: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262251: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262252: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262253: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262254: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262255: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262256: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262257: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262258: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262259: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262260: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262261: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262262: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262263: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262264: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262265: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262266: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262267: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262268: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262269: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262270: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262271: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262272: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262273: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262274: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262275: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262276: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262277: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262278: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262279: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262280: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262281: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262282: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262283: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262284: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262285: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262286: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262287: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262288: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262289: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262290: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262291: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262292: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262293: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262294: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262295: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262296: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262297: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262298: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262299: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262300: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262301: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262302: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262303: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262304: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262305: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262306: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262307: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262308: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262309: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262310: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262311: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262312: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262313: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262314: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262315: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262316: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262317: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262318: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262319: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262320: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262321: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262322: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262323: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262324: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262325: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262326: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262327: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262328: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262329: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262330: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262331: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262332: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262333: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262334: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262335: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262336: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262337: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262338: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262339: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262340: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262341: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262342: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262343: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262344: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262345: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262346: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262347: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262348: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262349: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262350: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262351: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262352: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262353: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262354: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262355: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262356: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262357: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262358: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262359: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262360: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262361: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262362: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262363: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262364: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262365: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262366: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262367: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262368: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262369: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262370: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262371: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262372: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262373: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262374: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262375: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262376: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262377: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262378: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262379: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262380: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262381: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262382: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262383: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262384: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262385: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262386: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262387: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262388: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262389: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262390: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262391: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262392: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262393: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262394: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262395: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262396: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262397: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262398: id is out of range, max=262143\n",
            "WARNING:hf-to-gguf:ignore token 262399: id is out of range, max=262143\n",
            "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 106\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{ '<start_of_turn>model\n",
            "' }}\n",
            "{%- endif -%}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:vicuna-13b-v1.5.gguf: n_tensors = 727, total_size = 4.7G\n",
            "Writing:  13% 597M/4.74G [00:29<02:01, 34.1Mbyte/s]^C\n"
          ]
        }
      ],
      "source": [
        "#Converting the HF model to GGUF model\n",
        "!python llama.cpp/convert_hf_to_gguf.py vicuna-hf \\\n",
        "  --outfile vicuna-13b-v1.5.gguf \\\n",
        "  --outtype q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KuMA0CkBZ-R",
        "outputId": "a133ce40-45e1-4c60-9cc3-0f045d186b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "576M -rw-r--r-- 1 root root 576M Aug  5 23:41 vicuna-13b-v1.5.gguf\n"
          ]
        }
      ],
      "source": [
        "#Verifying the GGUF model was created\n",
        "!ls -lash vicuna-13b-v1.5.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLWhPUW9CsgU"
      },
      "source": [
        "## Pushing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "1648218bd00e4cb692aff52824f71a5e",
            "399b5ebda18f41e284148014947bbea6",
            "55402f13c14a40d2b1c866bf987194cf",
            "7403c91eefdc499ba0d83c674673ead3",
            "f60f0c21c35b4ed2b9689da070cb40d3",
            "f863070d101846f09173911df8220007",
            "40e16e00a03a41ba93653c2ac49158db",
            "5609d1b50bd842388bc7f91887d906d4",
            "42cae4f962a247ffbf5004a71cb56595",
            "7da83eb1800749858d77e2247d81636a",
            "a14e12ff115c4167b39b4070545b33d9"
          ]
        },
        "id": "o-EI6t_fBjsa",
        "outputId": "f763e8a4-fbb7-4d4d-e2c4-a5e24c12233e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vicuna-13b-v1.5.gguf:   0%|          | 0.00/604M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1648218bd00e4cb692aff52824f71a5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/TechBuz/risk_predictor_ollama_model_q8_0/commit/f525809e0874528f18d626fb538e8d04bc97c9c3', commit_message='Upload vicuna-13b-v1.5.gguf with huggingface_hub', commit_description='', oid='f525809e0874528f18d626fb538e8d04bc97c9c3', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Pushing the GGUF model to HuggingFace\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"TechBuz/risk_predictor_ollama_model_q8_0\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"vicuna-13b-v1.5.gguf\",\n",
        "    path_in_repo=\"vicuna-13b-v1.5.gguf\",\n",
        "    repo_id=model_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"TechBuz/gemma-3N-risk_predictor\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\"TechBuz/gemma-3N-risk_predictor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "2c8a368adc8e40c3a5a10cc085220bc5",
            "47052680dc0f4bce824093c56ac2519a",
            "183524a7cabb4d69839481b7eff8ea89",
            "6cbd647a11c74e049a4539bc9a01541a",
            "424d298add0840c2b5cb3b49e6a2638d",
            "516c8f3b82be400f8b3ea69e16e0f9f5",
            "d4b758be08db49daae0ce469313e73bd",
            "1eabd1bcc02545d5879d0a905c319dbf",
            "f0a5af065d934d33b6f9a60023d24a73",
            "3930e238e95f415eae2b16cc2deef411",
            "ae92ca11d74d4ff984ec8abae74b9c4b",
            "1ba162a69a95471cae61be448dfe7d87",
            "ce3e3f3e0e214b6f912372a4aca9beb5",
            "4cbfd3866a01460191745a466d602175",
            "78d6ca3c0ffa4e7f8e9326d8d59f766e",
            "5cc8c5f20d084c459d36587d53e5a3b6",
            "39423c7a20974bb1b36a92958c8ff2b1",
            "0112c645d12040038f3cf31078093cf3",
            "2238d078af66489380615d46487fd2b9",
            "3ad6c6eaac164605b2164607e8736f44",
            "d7dbce56c47b463b9df7c8e41d110976",
            "3f3f2fb0e39b485e9e93b61585cf6708",
            "ea6a3b7fb82849a295697410e76070d7",
            "d8bf73dee29d41fcbddbbcfbe591f161",
            "b552f46d6d134e31a6ff4edb857d2540",
            "3a2a36d987944728b9291f00d50abcc9",
            "35bf64034eec42ecb6b671d923b21a35",
            "8764ffb229494caf995078deab6ca2ae",
            "3d5e8bb6a44943a796af2fbcc1402fe0",
            "84dd3926e62f4c419e3c47469f82ee0c",
            "7b94c3489c7e4d96bb03d42b13eb6bc7",
            "451cf96e58e04cb9a65bbb74db3590b2",
            "e765e9897ff849f7956bc2cac8a3f7ab",
            "23e9cd87b6be49f189d9a7a0dc789b78",
            "16ad25f9a57045dc81ba3cfacd425810",
            "90c2eb41124d43b78056771c9f8bbf3f",
            "c8a1cf9051144d5abb242fb5f6593fd9",
            "a22267a8859f4e96950f36509ee76531",
            "1bf04961622a47cb9bd3f9e9c06023ab",
            "8e61a28b20974f9dbc183fdd0b75eb8c",
            "375cc8ee9f7c47069488892a0ec55842",
            "00c82ed2524f464aa338c975a22d9d7f",
            "5c0949afa3f84ee99563fabc014a2d08",
            "17d3b54cae0f4e91a8f13c86475fa6b5",
            "d7d31dee4db0471b84d33a2db43fd2cf",
            "e318b9c9c83949c09183bbf242177925",
            "68d27d6663134e59a6dc7ff95d49bb7d",
            "b91a56b382254349b89bd992112f4bbb",
            "2498bced4eb04a5cb8651450505b4b89",
            "4789972004704be6bb56a16db0291e81",
            "2b83ab11ece546f5b53be511da7b9e6e",
            "00a0ddf4ab294722b0b3540973f2c57e",
            "3b00cba4f6854d82bb13ac0eada29ac2",
            "a9a135131df94fa28d0c0016e2b81404",
            "13ad4ea2cb554b4c956ccefd4d8281b6",
            "f0ca087f46a240ce86fb5a832bed3bc0",
            "20879c5bc0be4678b2079df00da13fe4",
            "f4e16ef91b6e4858b6384ffaa7909ece",
            "f21b4ce7fb5f4a65abef45476ed6d264",
            "5159b3c349144729b23150b43576de62",
            "c21893e081224ae3bad42c6544916971",
            "28358d603aad494db773b3652f79c29b",
            "ee203be335c345ae9d3a20a358d8dec5",
            "5b1dea16296c444d873f4557994fb09d",
            "5a43bfb2ef3d4f9189340c45cb0f1fcb",
            "1bd7206c4bc34aaab84d5c974d82aa65",
            "182387a77f8f440eb5d4218e6a545c20",
            "f0319c6c0d0b425d822d3e40db5b832b",
            "60b092fc2021499282a084cbbf072dee",
            "93d8486edb66443b945000990dd059e4",
            "d1bc190fa89f4cd989f3e6bae759d74e",
            "0ecbf0ae222c4296a220127bedd95cd5",
            "e5bf630438c54b85ab70762d7a455942",
            "ea36c463a38f4d5c9d1ffe7ce3119ee3",
            "5efd2688ec14454898b846212f47ea5d",
            "ef1aff2a9cf844f591ac2473af3c3b1c",
            "bbda99e46c0f43f497cd315887d7cfb3"
          ]
        },
        "id": "iDdRehgEx-pW",
        "outputId": "ade7fda0-54bc-491f-dddb-e567e1d196c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c8a368adc8e40c3a5a10cc085220bc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ba162a69a95471cae61be448dfe7d87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea6a3b7fb82849a295697410e76070d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23e9cd87b6be49f189d9a7a0dc789b78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7d31dee4db0471b84d33a2db43fd2cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0ca087f46a240ce86fb5a832bed3bc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "182387a77f8f440eb5d4218e6a545c20"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1648218bd00e4cb692aff52824f71a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_399b5ebda18f41e284148014947bbea6",
              "IPY_MODEL_55402f13c14a40d2b1c866bf987194cf",
              "IPY_MODEL_7403c91eefdc499ba0d83c674673ead3"
            ],
            "layout": "IPY_MODEL_f60f0c21c35b4ed2b9689da070cb40d3"
          }
        },
        "399b5ebda18f41e284148014947bbea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f863070d101846f09173911df8220007",
            "placeholder": "​",
            "style": "IPY_MODEL_40e16e00a03a41ba93653c2ac49158db",
            "value": "vicuna-13b-v1.5.gguf: 100%"
          }
        },
        "55402f13c14a40d2b1c866bf987194cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5609d1b50bd842388bc7f91887d906d4",
            "max": 603722144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42cae4f962a247ffbf5004a71cb56595",
            "value": 603722144
          }
        },
        "7403c91eefdc499ba0d83c674673ead3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7da83eb1800749858d77e2247d81636a",
            "placeholder": "​",
            "style": "IPY_MODEL_a14e12ff115c4167b39b4070545b33d9",
            "value": " 604M/604M [00:32&lt;00:00, 24.6MB/s]"
          }
        },
        "f60f0c21c35b4ed2b9689da070cb40d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f863070d101846f09173911df8220007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40e16e00a03a41ba93653c2ac49158db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5609d1b50bd842388bc7f91887d906d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42cae4f962a247ffbf5004a71cb56595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7da83eb1800749858d77e2247d81636a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14e12ff115c4167b39b4070545b33d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c8a368adc8e40c3a5a10cc085220bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47052680dc0f4bce824093c56ac2519a",
              "IPY_MODEL_183524a7cabb4d69839481b7eff8ea89",
              "IPY_MODEL_6cbd647a11c74e049a4539bc9a01541a"
            ],
            "layout": "IPY_MODEL_424d298add0840c2b5cb3b49e6a2638d"
          }
        },
        "47052680dc0f4bce824093c56ac2519a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_516c8f3b82be400f8b3ea69e16e0f9f5",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b758be08db49daae0ce469313e73bd",
            "value": "chat_template.jinja: "
          }
        },
        "183524a7cabb4d69839481b7eff8ea89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eabd1bcc02545d5879d0a905c319dbf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0a5af065d934d33b6f9a60023d24a73",
            "value": 1
          }
        },
        "6cbd647a11c74e049a4539bc9a01541a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3930e238e95f415eae2b16cc2deef411",
            "placeholder": "​",
            "style": "IPY_MODEL_ae92ca11d74d4ff984ec8abae74b9c4b",
            "value": " 1.53k/? [00:00&lt;00:00, 49.3kB/s]"
          }
        },
        "424d298add0840c2b5cb3b49e6a2638d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516c8f3b82be400f8b3ea69e16e0f9f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b758be08db49daae0ce469313e73bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eabd1bcc02545d5879d0a905c319dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f0a5af065d934d33b6f9a60023d24a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3930e238e95f415eae2b16cc2deef411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae92ca11d74d4ff984ec8abae74b9c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ba162a69a95471cae61be448dfe7d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce3e3f3e0e214b6f912372a4aca9beb5",
              "IPY_MODEL_4cbfd3866a01460191745a466d602175",
              "IPY_MODEL_78d6ca3c0ffa4e7f8e9326d8d59f766e"
            ],
            "layout": "IPY_MODEL_5cc8c5f20d084c459d36587d53e5a3b6"
          }
        },
        "ce3e3f3e0e214b6f912372a4aca9beb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39423c7a20974bb1b36a92958c8ff2b1",
            "placeholder": "​",
            "style": "IPY_MODEL_0112c645d12040038f3cf31078093cf3",
            "value": "preprocessor_config.json: "
          }
        },
        "4cbfd3866a01460191745a466d602175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2238d078af66489380615d46487fd2b9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ad6c6eaac164605b2164607e8736f44",
            "value": 1
          }
        },
        "78d6ca3c0ffa4e7f8e9326d8d59f766e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7dbce56c47b463b9df7c8e41d110976",
            "placeholder": "​",
            "style": "IPY_MODEL_3f3f2fb0e39b485e9e93b61585cf6708",
            "value": " 1.09k/? [00:00&lt;00:00, 34.2kB/s]"
          }
        },
        "5cc8c5f20d084c459d36587d53e5a3b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39423c7a20974bb1b36a92958c8ff2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0112c645d12040038f3cf31078093cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2238d078af66489380615d46487fd2b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3ad6c6eaac164605b2164607e8736f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7dbce56c47b463b9df7c8e41d110976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f3f2fb0e39b485e9e93b61585cf6708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea6a3b7fb82849a295697410e76070d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8bf73dee29d41fcbddbbcfbe591f161",
              "IPY_MODEL_b552f46d6d134e31a6ff4edb857d2540",
              "IPY_MODEL_3a2a36d987944728b9291f00d50abcc9"
            ],
            "layout": "IPY_MODEL_35bf64034eec42ecb6b671d923b21a35"
          }
        },
        "d8bf73dee29d41fcbddbbcfbe591f161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8764ffb229494caf995078deab6ca2ae",
            "placeholder": "​",
            "style": "IPY_MODEL_3d5e8bb6a44943a796af2fbcc1402fe0",
            "value": "model.safetensors.index.json: "
          }
        },
        "b552f46d6d134e31a6ff4edb857d2540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84dd3926e62f4c419e3c47469f82ee0c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b94c3489c7e4d96bb03d42b13eb6bc7",
            "value": 1
          }
        },
        "3a2a36d987944728b9291f00d50abcc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_451cf96e58e04cb9a65bbb74db3590b2",
            "placeholder": "​",
            "style": "IPY_MODEL_e765e9897ff849f7956bc2cac8a3f7ab",
            "value": " 159k/? [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "35bf64034eec42ecb6b671d923b21a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8764ffb229494caf995078deab6ca2ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5e8bb6a44943a796af2fbcc1402fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84dd3926e62f4c419e3c47469f82ee0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7b94c3489c7e4d96bb03d42b13eb6bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "451cf96e58e04cb9a65bbb74db3590b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e765e9897ff849f7956bc2cac8a3f7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23e9cd87b6be49f189d9a7a0dc789b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16ad25f9a57045dc81ba3cfacd425810",
              "IPY_MODEL_90c2eb41124d43b78056771c9f8bbf3f",
              "IPY_MODEL_c8a1cf9051144d5abb242fb5f6593fd9"
            ],
            "layout": "IPY_MODEL_a22267a8859f4e96950f36509ee76531"
          }
        },
        "16ad25f9a57045dc81ba3cfacd425810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf04961622a47cb9bd3f9e9c06023ab",
            "placeholder": "​",
            "style": "IPY_MODEL_8e61a28b20974f9dbc183fdd0b75eb8c",
            "value": "Fetching 3 files:   0%"
          }
        },
        "90c2eb41124d43b78056771c9f8bbf3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_375cc8ee9f7c47069488892a0ec55842",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00c82ed2524f464aa338c975a22d9d7f",
            "value": 0
          }
        },
        "c8a1cf9051144d5abb242fb5f6593fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0949afa3f84ee99563fabc014a2d08",
            "placeholder": "​",
            "style": "IPY_MODEL_17d3b54cae0f4e91a8f13c86475fa6b5",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "a22267a8859f4e96950f36509ee76531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf04961622a47cb9bd3f9e9c06023ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e61a28b20974f9dbc183fdd0b75eb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "375cc8ee9f7c47069488892a0ec55842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00c82ed2524f464aa338c975a22d9d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c0949afa3f84ee99563fabc014a2d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d3b54cae0f4e91a8f13c86475fa6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d31dee4db0471b84d33a2db43fd2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e318b9c9c83949c09183bbf242177925",
              "IPY_MODEL_68d27d6663134e59a6dc7ff95d49bb7d",
              "IPY_MODEL_b91a56b382254349b89bd992112f4bbb"
            ],
            "layout": "IPY_MODEL_2498bced4eb04a5cb8651450505b4b89"
          }
        },
        "e318b9c9c83949c09183bbf242177925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4789972004704be6bb56a16db0291e81",
            "placeholder": "​",
            "style": "IPY_MODEL_2b83ab11ece546f5b53be511da7b9e6e",
            "value": "model-00003-of-00003.safetensors:   5%"
          }
        },
        "68d27d6663134e59a6dc7ff95d49bb7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a0ddf4ab294722b0b3540973f2c57e",
            "max": 2820738448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b00cba4f6854d82bb13ac0eada29ac2",
            "value": 153297761
          }
        },
        "b91a56b382254349b89bd992112f4bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9a135131df94fa28d0c0016e2b81404",
            "placeholder": "​",
            "style": "IPY_MODEL_13ad4ea2cb554b4c956ccefd4d8281b6",
            "value": " 153M/2.82G [00:13&lt;02:04, 21.5MB/s]"
          }
        },
        "2498bced4eb04a5cb8651450505b4b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4789972004704be6bb56a16db0291e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b83ab11ece546f5b53be511da7b9e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a0ddf4ab294722b0b3540973f2c57e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b00cba4f6854d82bb13ac0eada29ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9a135131df94fa28d0c0016e2b81404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ad4ea2cb554b4c956ccefd4d8281b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0ca087f46a240ce86fb5a832bed3bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20879c5bc0be4678b2079df00da13fe4",
              "IPY_MODEL_f4e16ef91b6e4858b6384ffaa7909ece",
              "IPY_MODEL_f21b4ce7fb5f4a65abef45476ed6d264"
            ],
            "layout": "IPY_MODEL_5159b3c349144729b23150b43576de62"
          }
        },
        "20879c5bc0be4678b2079df00da13fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c21893e081224ae3bad42c6544916971",
            "placeholder": "​",
            "style": "IPY_MODEL_28358d603aad494db773b3652f79c29b",
            "value": "model-00001-of-00003.safetensors:   4%"
          }
        },
        "f4e16ef91b6e4858b6384ffaa7909ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee203be335c345ae9d3a20a358d8dec5",
            "max": 3077103824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b1dea16296c444d873f4557994fb09d",
            "value": 134088073
          }
        },
        "f21b4ce7fb5f4a65abef45476ed6d264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a43bfb2ef3d4f9189340c45cb0f1fcb",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd7206c4bc34aaab84d5c974d82aa65",
            "value": " 134M/3.08G [00:15&lt;04:55, 9.96MB/s]"
          }
        },
        "5159b3c349144729b23150b43576de62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21893e081224ae3bad42c6544916971": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28358d603aad494db773b3652f79c29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee203be335c345ae9d3a20a358d8dec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1dea16296c444d873f4557994fb09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a43bfb2ef3d4f9189340c45cb0f1fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd7206c4bc34aaab84d5c974d82aa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "182387a77f8f440eb5d4218e6a545c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0319c6c0d0b425d822d3e40db5b832b",
              "IPY_MODEL_60b092fc2021499282a084cbbf072dee",
              "IPY_MODEL_93d8486edb66443b945000990dd059e4"
            ],
            "layout": "IPY_MODEL_d1bc190fa89f4cd989f3e6bae759d74e"
          }
        },
        "f0319c6c0d0b425d822d3e40db5b832b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ecbf0ae222c4296a220127bedd95cd5",
            "placeholder": "​",
            "style": "IPY_MODEL_e5bf630438c54b85ab70762d7a455942",
            "value": "model-00002-of-00003.safetensors:  16%"
          }
        },
        "60b092fc2021499282a084cbbf072dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea36c463a38f4d5c9d1ffe7ce3119ee3",
            "max": 4981242104,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5efd2688ec14454898b846212f47ea5d",
            "value": 805543182
          }
        },
        "93d8486edb66443b945000990dd059e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1aff2a9cf844f591ac2473af3c3b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_bbda99e46c0f43f497cd315887d7cfb3",
            "value": " 806M/4.98G [00:12&lt;00:49, 84.1MB/s]"
          }
        },
        "d1bc190fa89f4cd989f3e6bae759d74e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecbf0ae222c4296a220127bedd95cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5bf630438c54b85ab70762d7a455942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea36c463a38f4d5c9d1ffe7ce3119ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5efd2688ec14454898b846212f47ea5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef1aff2a9cf844f591ac2473af3c3b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbda99e46c0f43f497cd315887d7cfb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}